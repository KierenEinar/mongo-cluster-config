2018-02-28T14:15:18.073+0800 I CONTROL  [initandlisten] MongoDB starting : pid=30769 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-02-28T14:15:18.074+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-02-28T14:15:18.074+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-02-28T14:15:18.074+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-02-28T14:15:18.074+0800 I CONTROL  [initandlisten] allocator: system
2018-02-28T14:15:18.074+0800 I CONTROL  [initandlisten] modules: none
2018-02-28T14:15:18.074+0800 I CONTROL  [initandlisten] build environment:
2018-02-28T14:15:18.074+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-02-28T14:15:18.074+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-02-28T14:15:18.074+0800 I CONTROL  [initandlisten] options: { config: "mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-02-28T14:15:18.075+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-02-28T14:15:19.658+0800 I CONTROL  [initandlisten] 
2018-02-28T14:15:19.658+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-02-28T14:15:19.658+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-02-28T14:15:19.658+0800 I CONTROL  [initandlisten] 
2018-02-28T14:15:19.658+0800 I CONTROL  [initandlisten] 
2018-02-28T14:15:19.658+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-02-28T14:15:20.368+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-02-28T14:15:20.373+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-02-28T14:15:20.373+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T14:15:20.479+0800 I REPL     [initandlisten] Did not find local voted for document at startup.
2018-02-28T14:15:20.479+0800 I REPL     [initandlisten] Did not find local replica set configuration document at startup;  NoMatchingDocument: Did not find replica set configuration document in local.system.replset
2018-02-28T14:15:20.479+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-02-28T14:15:50.379+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T14:16:20.385+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T14:16:50.388+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T14:17:20.392+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T14:17:45.678+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:49697 #1 (1 connection now open)
2018-02-28T14:17:45.679+0800 I -        [conn1] end connection 127.0.0.1:49697 (1 connection now open)
2018-02-28T14:17:45.680+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:49698 #2 (1 connection now open)
2018-02-28T14:17:45.680+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:49698 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T14:17:45.680+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T14:17:45.681+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T14:17:46.296+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:49701 #3 (2 connections now open)
2018-02-28T14:17:46.296+0800 I -        [conn3] end connection 127.0.0.1:49701 (2 connections now open)
2018-02-28T14:17:46.535+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:49702 #4 (2 connections now open)
2018-02-28T14:17:46.535+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:49702 conn4: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T14:17:46.536+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T14:17:46.536+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 0ms (1 connections now open to 127.0.0.1:28002)
2018-02-28T14:17:46.663+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-02-28T14:17:46.663+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-02-28T14:17:46.937+0800 I REPL     [replication-0] Starting initial sync (attempt 1 of 10)
2018-02-28T14:17:46.937+0800 I REPL     [ReplicationExecutor] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-02-28T14:17:46.937+0800 I REPL     [ReplicationExecutor] This node is 127.0.0.1:28003 in the config
2018-02-28T14:17:46.937+0800 I REPL     [ReplicationExecutor] transition to STARTUP2
2018-02-28T14:17:46.937+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-02-28T14:17:46.937+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state STARTUP2
2018-02-28T14:17:47.086+0800 I REPL     [replication-0] sync source candidate: 127.0.0.1:28001
2018-02-28T14:17:47.086+0800 I STORAGE  [replication-0] dropAllDatabasesExceptLocal 1
2018-02-28T14:17:47.086+0800 I REPL     [replication-0] ******
2018-02-28T14:17:47.086+0800 I REPL     [replication-0] creating replication oplog of size: 192MB...
2018-02-28T14:17:47.161+0800 I STORAGE  [replication-0] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-02-28T14:17:47.161+0800 I STORAGE  [replication-0] The size storer reports that the oplog contains 0 records totaling to 0 bytes
2018-02-28T14:17:47.162+0800 I STORAGE  [replication-0] Scanning the oplog to determine where to place markers for truncation
2018-02-28T14:17:47.424+0800 I REPL     [replication-0] ******
2018-02-28T14:17:47.424+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T14:17:47.425+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T14:17:47.425+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T14:17:47.426+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (2 connections now open to 127.0.0.1:28001)
2018-02-28T14:17:47.426+0800 I REPL     [replication-1] CollectionCloner::start called, on ns:admin.system.version
2018-02-28T14:17:47.477+0800 I INDEX    [InitialSyncInserters-admin.system.version0] build index on: admin.system.version properties: { v: 2, key: { version: 1 }, name: "incompatible_with_version_32", ns: "admin.system.version" }
2018-02-28T14:17:47.478+0800 I INDEX    [InitialSyncInserters-admin.system.version0] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:17:47.504+0800 I INDEX    [InitialSyncInserters-admin.system.version0] build index on: admin.system.version properties: { v: 1, key: { _id: 1 }, name: "_id_", ns: "admin.system.version" }
2018-02-28T14:17:47.504+0800 I INDEX    [InitialSyncInserters-admin.system.version0] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:17:47.504+0800 I COMMAND  [InitialSyncInserters-admin.system.version0] setting featureCompatibilityVersion to 3.4
2018-02-28T14:17:47.546+0800 I REPL     [replication-1] No need to apply operations. (currently at { : Timestamp 1519798665000|1 })
2018-02-28T14:17:47.546+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28001 due to bad connection status; 1 connections to that host remain open
2018-02-28T14:17:47.546+0800 I REPL     [replication-1] Finished fetching oplog during initial sync: CallbackCanceled: Callback canceled. Last fetched optime and hash: { ts: Timestamp 1519798665000|1, t: -1 }[-8629422867872974848]
2018-02-28T14:17:47.547+0800 I REPL     [replication-1] Initial sync attempt finishing up.
2018-02-28T14:17:47.547+0800 I REPL     [replication-1] Initial Sync Attempt Statistics: { failedInitialSyncAttempts: 0, maxFailedInitialSyncAttempts: 10, initialSyncStart: new Date(1519798666937), initialSyncAttempts: [], fetchedMissingDocs: 0, appliedOps: 0, initialSyncOplogStart: Timestamp 1519798665000|1, initialSyncOplogEnd: Timestamp 1519798665000|1, databases: { databasesCloned: 1, admin: { collections: 1, clonedCollections: 1, start: new Date(1519798667426), end: new Date(1519798667546), elapsedMillis: 120, admin.system.version: { documentsToCopy: 1, documentsCopied: 1, indexes: 2, fetchedBatches: 1, start: new Date(1519798667426), end: new Date(1519798667546), elapsedMillis: 120 } } } }
2018-02-28T14:17:47.600+0800 I REPL     [replication-1] initial sync done; took 0s.
2018-02-28T14:17:47.600+0800 I REPL     [replication-1] Starting replication fetcher thread
2018-02-28T14:17:47.600+0800 I REPL     [replication-1] Starting replication applier thread
2018-02-28T14:17:47.600+0800 I REPL     [replication-1] Starting replication reporter thread
2018-02-28T14:17:47.600+0800 I REPL     [rsSync] transition to RECOVERING
2018-02-28T14:17:47.600+0800 I REPL     [rsBackgroundSync] could not find member to sync from
2018-02-28T14:17:47.601+0800 I REPL     [rsSync] transition to SECONDARY
2018-02-28T14:17:47.601+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-02-28T14:17:50.397+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T14:17:57.484+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:49732 #5 (3 connections now open)
2018-02-28T14:17:57.484+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:49732 conn5: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T14:17:57.492+0800 I COMMAND  [conn2] command local.replset.election command: replSetRequestVotes { replSetRequestVotes: 1, setName: "configset", dryRun: false, term: 1, candidateIndex: 0, configVersion: 1, lastCommittedOp: { ts: Timestamp 1519798665000|1, t: -1 } } numYields:0 reslen:123 locks:{ Global: { acquireCount: { r: 4, w: 2 } }, Database: { acquireCount: { r: 1, W: 2 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 119ms
2018-02-28T14:17:57.492+0800 I -        [conn2] end connection 127.0.0.1:49698 (3 connections now open)
2018-02-28T14:17:57.606+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-02-28T14:18:02.662+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28002
2018-02-28T14:18:02.662+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-02-28T14:18:02.662+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 0ms (1 connections now open to 127.0.0.1:28002)
2018-02-28T14:18:02.704+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-02-28T14:18:02.704+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 0ms (2 connections now open to 127.0.0.1:28002)
2018-02-28T14:18:02.886+0800 I INDEX    [repl writer worker 5] build index on: config.chunks properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.chunks" }
2018-02-28T14:18:02.886+0800 I INDEX    [repl writer worker 5] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:18:02.905+0800 I INDEX    [repl writer worker 5] build index done.  scanned 0 total records. 0 secs
2018-02-28T14:18:02.957+0800 I INDEX    [repl writer worker 7] build index on: config.chunks properties: { v: 2, unique: true, key: { ns: 1, shard: 1, min: 1 }, name: "ns_1_shard_1_min_1", ns: "config.chunks" }
2018-02-28T14:18:02.957+0800 I INDEX    [repl writer worker 7] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:18:02.984+0800 I INDEX    [repl writer worker 7] build index done.  scanned 0 total records. 0 secs
2018-02-28T14:18:03.040+0800 I INDEX    [repl writer worker 9] build index on: config.chunks properties: { v: 2, unique: true, key: { ns: 1, lastmod: 1 }, name: "ns_1_lastmod_1", ns: "config.chunks" }
2018-02-28T14:18:03.040+0800 I INDEX    [repl writer worker 9] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:18:03.068+0800 I INDEX    [repl writer worker 9] build index done.  scanned 0 total records. 0 secs
2018-02-28T14:18:03.278+0800 I INDEX    [repl writer worker 15] build index on: config.migrations properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.migrations" }
2018-02-28T14:18:03.278+0800 I INDEX    [repl writer worker 15] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:18:03.298+0800 I INDEX    [repl writer worker 15] build index done.  scanned 0 total records. 0 secs
2018-02-28T14:18:03.420+0800 I INDEX    [repl writer worker 1] build index on: config.shards properties: { v: 2, unique: true, key: { host: 1 }, name: "host_1", ns: "config.shards" }
2018-02-28T14:18:03.420+0800 I INDEX    [repl writer worker 1] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:18:03.436+0800 I INDEX    [repl writer worker 1] build index done.  scanned 0 total records. 0 secs
2018-02-28T14:18:03.602+0800 I INDEX    [repl writer worker 5] build index on: config.locks properties: { v: 2, key: { ts: 1 }, name: "ts_1", ns: "config.locks" }
2018-02-28T14:18:03.603+0800 I INDEX    [repl writer worker 5] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:18:03.623+0800 I INDEX    [repl writer worker 5] build index done.  scanned 0 total records. 0 secs
2018-02-28T14:18:03.666+0800 I INDEX    [repl writer worker 7] build index on: config.locks properties: { v: 2, key: { state: 1, process: 1 }, name: "state_1_process_1", ns: "config.locks" }
2018-02-28T14:18:03.666+0800 I INDEX    [repl writer worker 7] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:18:03.684+0800 I INDEX    [repl writer worker 7] build index done.  scanned 0 total records. 0 secs
2018-02-28T14:18:03.808+0800 I INDEX    [repl writer worker 11] build index on: config.lockpings properties: { v: 2, key: { ping: 1 }, name: "ping_1", ns: "config.lockpings" }
2018-02-28T14:18:03.809+0800 I INDEX    [repl writer worker 11] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:18:03.828+0800 I INDEX    [repl writer worker 11] build index done.  scanned 0 total records. 0 secs
2018-02-28T14:18:03.951+0800 I INDEX    [repl writer worker 14] build index on: config.tags properties: { v: 2, unique: true, key: { ns: 1, min: 1 }, name: "ns_1_min_1", ns: "config.tags" }
2018-02-28T14:18:03.951+0800 I INDEX    [repl writer worker 14] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:18:03.960+0800 I INDEX    [repl writer worker 14] build index done.  scanned 0 total records. 0 secs
2018-02-28T14:18:04.282+0800 I INDEX    [repl writer worker 1] build index on: config.tags properties: { v: 2, key: { ns: 1, tag: 1 }, name: "ns_1_tag_1", ns: "config.tags" }
2018-02-28T14:18:04.282+0800 I INDEX    [repl writer worker 1] 	 building index using bulk method; build may temporarily use up to 500 megabytes of RAM
2018-02-28T14:18:04.293+0800 I INDEX    [repl writer worker 1] build index done.  scanned 0 total records. 0 secs
2018-02-28T14:22:06.604+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50167 #6 (3 connections now open)
2018-02-28T14:22:06.605+0800 I NETWORK  [conn6] received client metadata from 127.0.0.1:50167 conn6: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T14:22:06.607+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50169 #7 (4 connections now open)
2018-02-28T14:22:06.607+0800 I NETWORK  [conn7] received client metadata from 127.0.0.1:50169 conn7: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T14:22:13.733+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519798928000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4981ms
2018-02-28T14:22:28.747+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519798943000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4985ms
2018-02-28T14:23:23.872+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519798998000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4990ms
2018-02-28T14:23:36.749+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799013000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2847ms
2018-02-28T14:23:51.763+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799026000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4983ms
2018-02-28T14:24:06.778+0800 I COMMAND  [conn7] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799041000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:352 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 120ms
2018-02-28T14:24:16.747+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799051000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4935ms
2018-02-28T14:24:20.451+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-02-28T14:24:20.452+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-02-28T14:24:20.452+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-02-28T14:24:20.454+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-02-28T14:24:31.782+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799066000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4985ms
2018-02-28T14:24:41.788+0800 I COMMAND  [conn7] command config.collections command: find { find: "collections", filter: { _id: /^config\./ }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799076000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:357 locks:{ Global: { acquireCount: { r: 2 }, acquireWaitCount: { r: 1 }, timeAcquiringMicros: { r: 197 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1632ms
2018-02-28T14:24:44.243+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799081000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2438ms
2018-02-28T14:24:50.457+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-02-28T14:24:50.460+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-02-28T14:24:50.461+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-02-28T14:24:50.461+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-02-28T14:24:59.271+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799094000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5000ms
2018-02-28T14:25:14.283+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799109000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4995ms
2018-02-28T14:27:06.965+0800 I COMMAND  [conn7] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799224000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 154ms
2018-02-28T14:28:30.074+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799305000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4976ms
2018-02-28T14:28:45.089+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799320000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4982ms
2018-02-28T14:29:06.986+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50975 #8 (5 connections now open)
2018-02-28T14:29:06.986+0800 I NETWORK  [conn8] received client metadata from 127.0.0.1:50975 conn8: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T14:29:07.094+0800 I COMMAND  [conn7] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799345000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1851ms
2018-02-28T14:29:07.094+0800 I COMMAND  [conn8] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799345000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 107ms
2018-02-28T14:29:22.120+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799357000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4986ms
2018-02-28T14:29:35.338+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799372000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3194ms
2018-02-28T14:29:47.073+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799385000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1703ms
2018-02-28T14:30:02.088+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799397000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4986ms
2018-02-28T14:30:07.114+0800 I -        [conn7] end connection 127.0.0.1:50169 (5 connections now open)
2018-02-28T14:30:25.484+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799422000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3028ms
2018-02-28T14:31:00.591+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799455000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4979ms
2018-02-28T14:31:15.608+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799470000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4992ms
2018-02-28T14:33:17.260+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799596000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1178ms
2018-02-28T14:33:32.273+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799607000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4983ms
2018-02-28T14:33:46.177+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799622000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3880ms
2018-02-28T14:38:47.542+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799927000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 351ms
2018-02-28T14:39:02.551+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799937000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4994ms
2018-02-28T14:39:27.327+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519799962000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4724ms
2018-02-28T14:41:32.711+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800087000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5013ms
2018-02-28T14:42:07.778+0800 I COMMAND  [conn8] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800122000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 564ms
2018-02-28T14:42:17.749+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800132000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4919ms
2018-02-28T14:43:23.024+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800197000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4990ms
2018-02-28T14:43:47.836+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800223000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4718ms
2018-02-28T14:43:58.180+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800237000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 307ms
2018-02-28T14:45:33.466+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800328000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5003ms
2018-02-28T14:45:47.965+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800343000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4465ms
2018-02-28T14:46:18.588+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800378000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 479ms
2018-02-28T14:46:33.613+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800388000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4931ms
2018-02-28T14:46:58.649+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800413000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4989ms
2018-02-28T14:47:18.846+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800438000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 156ms
2018-02-28T14:47:33.884+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800448000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4757ms
2018-02-28T14:47:48.084+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800463000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4174ms
2018-02-28T14:48:19.275+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800498000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1042ms
2018-02-28T14:49:04.394+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800539000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4992ms
2018-02-28T14:49:37.899+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:53251 #9 (5 connections now open)
2018-02-28T14:49:37.901+0800 I NETWORK  [conn9] received client metadata from 127.0.0.1:53251 conn9: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T14:49:38.247+0800 I COMMAND  [conn8] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800574000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3754ms
2018-02-28T14:49:38.247+0800 I COMMAND  [conn9] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800574000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 346ms
2018-02-28T14:49:49.546+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800588000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1266ms
2018-02-28T14:50:04.585+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800599000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5003ms
2018-02-28T14:50:18.222+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800614000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3607ms
2018-02-28T14:50:29.678+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800628000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1422ms
2018-02-28T14:50:38.276+0800 I -        [conn8] end connection 127.0.0.1:50975 (5 connections now open)
2018-02-28T14:52:04.972+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800719000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4995ms
2018-02-28T14:52:30.021+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800745000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4987ms
2018-02-28T14:54:45.763+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800880000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4814ms
2018-02-28T14:55:00.774+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800895000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4976ms
2018-02-28T14:55:11.062+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800910000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 256ms
2018-02-28T14:56:16.436+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800971000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4991ms
2018-02-28T14:56:31.450+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519800986000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4967ms
2018-02-28T14:57:01.686+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801021000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 130ms
2018-02-28T14:57:36.851+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801051000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4980ms
2018-02-28T14:57:48.683+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801066000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1803ms
2018-02-28T14:58:01.980+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801078000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3229ms
2018-02-28T15:01:27.659+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801282000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4997ms
2018-02-28T15:01:39.009+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801297000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1327ms
2018-02-28T15:01:52.756+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801309000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3714ms
2018-02-28T15:05:18.467+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801513000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4957ms
2018-02-28T15:05:49.146+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801548000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 583ms
2018-02-28T15:06:03.693+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801559000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4475ms
2018-02-28T15:06:18.721+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801573000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4989ms
2018-02-28T15:06:33.733+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801588000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4984ms
2018-02-28T15:06:54.135+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801613000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 333ms
2018-02-28T15:07:09.321+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801624000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4976ms
2018-02-28T15:07:34.364+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801649000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4992ms
2018-02-28T15:11:00.121+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801855000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4957ms
2018-02-28T15:11:15.134+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801870000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4978ms
2018-02-28T15:11:25.288+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519801885000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 119ms
2018-02-28T15:13:39.729+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802015000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3983ms
2018-02-28T15:13:49.930+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802029000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 184ms
2018-02-28T15:14:04.940+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802039000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4990ms
2018-02-28T15:14:15.850+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802054000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 889ms
2018-02-28T15:14:49.980+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802085000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4054ms
2018-02-28T15:15:04.994+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802099000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4992ms
2018-02-28T15:15:15.999+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802114000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 981ms
2018-02-28T15:15:31.038+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802126000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4998ms
2018-02-28T15:15:46.050+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802141000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4987ms
2018-02-28T15:16:09.828+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802166000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3673ms
2018-02-28T15:16:34.860+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802189000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4989ms
2018-02-28T15:16:56.366+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802214000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1457ms
2018-02-28T15:17:20.380+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802236000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3943ms
2018-02-28T15:17:35.395+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802250000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4990ms
2018-02-28T15:17:46.518+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802265000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1090ms
2018-02-28T15:18:20.420+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802296000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3798ms
2018-02-28T15:18:35.434+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802310000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4986ms
2018-02-28T15:18:46.704+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802325000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1239ms
2018-02-28T15:23:38.600+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:57220 #10 (5 connections now open)
2018-02-28T15:23:38.601+0800 I NETWORK  [conn10] received client metadata from 127.0.0.1:57220 conn10: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T15:23:40.510+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802617000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2632ms
2018-02-28T15:23:40.510+0800 I COMMAND  [conn10] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802617000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1907ms
2018-02-28T15:23:50.730+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802630000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 184ms
2018-02-28T15:24:05.743+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802640000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4996ms
2018-02-28T15:24:18.001+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802655000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2235ms
2018-02-28T15:24:40.552+0800 I -        [conn10] end connection 127.0.0.1:57220 (5 connections now open)
2018-02-28T15:24:40.552+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:57335 #11 (5 connections now open)
2018-02-28T15:24:40.553+0800 I NETWORK  [conn11] received client metadata from 127.0.0.1:57335 conn11: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T15:24:40.592+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802678000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2521ms
2018-02-28T15:24:50.777+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802690000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 166ms
2018-02-28T15:25:15.805+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802710000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4983ms
2018-02-28T15:25:38.277+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802735000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2407ms
2018-02-28T15:25:40.630+0800 I -        [conn11] end connection 127.0.0.1:57335 (5 connections now open)
2018-02-28T15:26:20.863+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802778000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2439ms
2018-02-28T15:26:45.899+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802800000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4982ms
2018-02-28T15:27:08.576+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802825000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2607ms
2018-02-28T15:29:03.987+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802938000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5003ms
2018-02-28T15:29:19.000+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802953000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4975ms
2018-02-28T15:30:04.224+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519802999000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5002ms
2018-02-28T15:30:19.239+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803014000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4980ms
2018-02-28T15:30:40.972+0800 I COMMAND  [conn9] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803039000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 147ms
2018-02-28T15:31:21.220+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803079000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1682ms
2018-02-28T15:31:46.265+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803101000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4985ms
2018-02-28T15:31:59.693+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803116000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3400ms
2018-02-28T15:33:21.318+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803199000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1332ms
2018-02-28T15:33:36.329+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803211000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4990ms
2018-02-28T15:33:50.088+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803226000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3740ms
2018-02-28T15:34:51.408+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803290000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1096ms
2018-02-28T15:35:16.476+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803311000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4973ms
2018-02-28T15:35:30.730+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803326000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4231ms
2018-02-28T15:35:41.265+0800 I COMMAND  [conn9] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803340000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 235ms
2018-02-28T15:39:46.661+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803581000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4991ms
2018-02-28T15:40:01.675+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803596000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4993ms
2018-02-28T15:45:17.783+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803912000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5001ms
2018-02-28T15:45:32.795+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519803927000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4980ms
2018-02-28T15:51:59.104+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804314000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5003ms
2018-02-28T15:52:12.362+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804329000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3240ms
2018-02-28T15:52:34.219+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804352000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1800ms
2018-02-28T15:53:09.315+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804384000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4990ms
2018-02-28T15:53:22.446+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804399000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3112ms
2018-02-28T15:53:34.424+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804412000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1939ms
2018-02-28T15:54:29.585+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804464000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5001ms
2018-02-28T15:54:42.487+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804479000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2875ms
2018-02-28T15:55:04.749+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804502000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2209ms
2018-02-28T15:55:29.818+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804524000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4991ms
2018-02-28T15:55:42.541+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804539000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2702ms
2018-02-28T15:56:04.968+0800 I COMMAND  [conn9] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804562000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2372ms
2018-02-28T15:58:35.448+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:61640 #12 (5 connections now open)
2018-02-28T15:58:35.453+0800 I NETWORK  [conn12] received client metadata from 127.0.0.1:61640 conn12: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T15:59:35.738+0800 I -        [conn9] end connection 127.0.0.1:53251 (5 connections now open)
2018-02-28T15:59:40.747+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804775000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4918ms
2018-02-28T15:59:52.778+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804790000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2002ms
2018-02-28T16:00:05.859+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804802000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3053ms
2018-02-28T16:00:52.549+0800 I COMMAND  [conn12] command config.databases command: find { find: "databases", filter: { _id: /^mytest$/i }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519804847000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:355 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4944ms
2018-02-28T16:03:41.520+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805016000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4992ms
2018-02-28T16:03:53.003+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805031000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1457ms
2018-02-28T16:04:16.643+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805053000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3583ms
2018-02-28T16:05:11.807+0800 I COMMAND  [conn12] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805106000|2, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 230ms
2018-02-28T16:08:12.463+0800 I COMMAND  [conn12] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805287000|2, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 606ms
2018-02-28T16:08:32.522+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805307000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5001ms
2018-02-28T16:08:43.225+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805322000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 678ms
2018-02-28T16:09:17.711+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805353000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4277ms
2018-02-28T16:11:03.099+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805458000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4996ms
2018-02-28T16:11:13.108+0800 I COMMAND  [conn12] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805468000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 619ms
2018-02-28T16:11:23.519+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805483000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 358ms
2018-02-28T16:11:38.250+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805493000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4689ms
2018-02-28T16:12:23.372+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805538000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4999ms
2018-02-28T16:12:38.386+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805553000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4980ms
2018-02-28T16:12:43.447+0800 I COMMAND  [conn12] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805558000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 327ms
2018-02-28T16:13:23.574+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805598000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4988ms
2018-02-28T16:13:43.576+0800 I COMMAND  [conn12] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805618000|1, t: 1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 118ms
2018-02-28T16:13:48.624+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519805623000|2, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4983ms
2018-02-28T16:20:24.145+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519806020000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4016ms
2018-02-28T16:20:39.158+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519806034000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4935ms
2018-02-28T16:21:00.256+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519806059000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1041ms
2018-02-28T16:22:35.562+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519806150000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4989ms
2018-02-28T16:22:50.577+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519806165000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4992ms
2018-02-28T16:25:21.167+0800 I NETWORK  [PeriodicTaskRunner] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27001)
2018-02-28T16:25:21.673+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27001, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:25:21.673+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27001 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27001 : couldn't connect to server 127.0.0.1:27001, connection attempt failed
2018-02-28T16:25:21.674+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] No primary detected for set set1
2018-02-28T16:25:24.417+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519806321000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3229ms
2018-02-28T16:25:39.425+0800 I COMMAND  [conn12] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519806334000|1, t: 1 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4987ms
2018-02-28T16:25:47.140+0800 I -        [conn5] end connection 127.0.0.1:49732 (4 connections now open)
2018-02-28T16:25:48.205+0800 I ASIO     [ReplicationExecutor] dropping unhealthy pooled connection to 127.0.0.1:28001
2018-02-28T16:25:48.205+0800 I ASIO     [ReplicationExecutor] after drop, pool was empty, going to spawn some connections
2018-02-28T16:25:48.205+0800 I ASIO     [ReplicationExecutor] Failed to close stream: Socket is not connected
2018-02-28T16:25:48.205+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:25:48.206+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:25:48.206+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:25:48.206+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:25:48.206+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:25:48.206+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:25:48.206+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:25:48.206+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:25:48.206+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:25:48.207+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:25:48.207+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:25:48.207+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:25:49.338+0800 I REPL     [replication-1] Choosing new sync source because our current sync source, 127.0.0.1:28002, has an OpTime ({ ts: Timestamp 1519806344000|1, t: 1 }) which is not ahead of ours ({ ts: Timestamp 1519806344000|1, t: 1 }), it does not have a sync source, and it's not the primary (sync source does not know the primary)
2018-02-28T16:25:49.338+0800 I REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 127.0.0.1:28002, OpTime { ts: Timestamp 1519806344000|1, t: 1 }, its sync source index:-1
2018-02-28T16:25:49.338+0800 W REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 127.0.0.1:28002 (config version: 1; last applied optime: { ts: Timestamp 1519806344000|1, t: 1 }; sync source index: -1; primary index: -1) is no longer valid
2018-02-28T16:25:49.338+0800 I REPL     [rsBackgroundSync] could not find member to sync from
2018-02-28T16:25:49.338+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:25:49.339+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:25:49.339+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:25:49.339+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:25:49.339+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:25:49.339+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:25:49.339+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:25:49.339+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:25:49.339+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:25:49.340+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:25:49.340+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:25:49.340+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:25:49.376+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28002: InvalidSyncSource: Sync source was cleared. Was 127.0.0.1:28002
2018-02-28T16:25:51.368+0800 I -        [conn4] end connection 127.0.0.1:49702 (3 connections now open)
2018-02-28T16:25:51.678+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27004)
2018-02-28T16:25:51.679+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27004, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:25:51.679+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27004 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27004 : couldn't connect to server 127.0.0.1:27004, connection attempt failed
2018-02-28T16:25:51.679+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27005)
2018-02-28T16:25:51.679+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27005, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:25:51.679+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27005 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27005 : couldn't connect to server 127.0.0.1:27005, connection attempt failed
2018-02-28T16:25:51.679+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27006)
2018-02-28T16:25:51.680+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27006, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:25:51.680+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27006 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27006 : couldn't connect to server 127.0.0.1:27006, connection attempt failed
2018-02-28T16:25:51.680+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] No primary detected for set set2
2018-02-28T16:25:51.680+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] All nodes for set set2 are down. This has happened for 1 checks in a row.
2018-02-28T16:25:51.680+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27001, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:25:51.680+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27002)
2018-02-28T16:25:51.680+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27002, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:25:51.680+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27002 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27002 : couldn't connect to server 127.0.0.1:27002, connection attempt failed
2018-02-28T16:25:51.680+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27003)
2018-02-28T16:25:51.680+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27003, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:25:51.680+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27003 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27003 : couldn't connect to server 127.0.0.1:27003, connection attempt failed
2018-02-28T16:25:51.680+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] No primary detected for set set1
2018-02-28T16:25:51.680+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] All nodes for set set1 are down. This has happened for 1 checks in a row.
2018-02-28T16:25:54.341+0800 I ASIO     [ReplicationExecutor] dropping unhealthy pooled connection to 127.0.0.1:28002
2018-02-28T16:25:54.341+0800 I ASIO     [ReplicationExecutor] after drop, pool was empty, going to spawn some connections
2018-02-28T16:25:54.341+0800 I ASIO     [ReplicationExecutor] Failed to close stream: Socket is not connected
2018-02-28T16:25:54.341+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28002 - HostUnreachable: Connection refused
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28002 due to failed operation on a connection
2018-02-28T16:25:54.342+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28002; HostUnreachable: Connection refused
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T16:25:54.342+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28002 - HostUnreachable: Connection refused
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28002 due to failed operation on a connection
2018-02-28T16:25:54.342+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28002; HostUnreachable: Connection refused
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T16:25:54.342+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:25:54.342+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:25:54.343+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28002 - HostUnreachable: Connection refused
2018-02-28T16:25:54.343+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28002 due to failed operation on a connection
2018-02-28T16:25:54.343+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28002; HostUnreachable: Connection refused
2018-02-28T16:25:54.343+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:25:54.343+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:25:54.343+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:25:56.266+0800 I REPL     [ReplicationExecutor] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2018-02-28T16:39:23.527+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-02-28T16:39:23.552+0800 I CONTROL  [initandlisten] MongoDB starting : pid=39263 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-02-28T16:39:23.552+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-02-28T16:39:23.552+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-02-28T16:39:23.552+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-02-28T16:39:23.552+0800 I CONTROL  [initandlisten] allocator: system
2018-02-28T16:39:23.552+0800 I CONTROL  [initandlisten] modules: none
2018-02-28T16:39:23.552+0800 I CONTROL  [initandlisten] build environment:
2018-02-28T16:39:23.552+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-02-28T16:39:23.552+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-02-28T16:39:23.552+0800 I CONTROL  [initandlisten] options: { config: "node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-02-28T16:39:23.552+0800 W -        [initandlisten] Detected unclean shutdown - /usr/local/mongo-cluster-sharding/configsvr/node3/data/db/mongod.lock is not empty.
2018-02-28T16:39:23.553+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-02-28T16:39:23.553+0800 W STORAGE  [initandlisten] Recovering data from the last clean checkpoint.
2018-02-28T16:39:23.553+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-02-28T16:39:25.205+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-02-28T16:39:25.205+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 1955 records totaling to 476088 bytes
2018-02-28T16:39:25.205+0800 I STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2018-02-28T16:39:25.380+0800 I CONTROL  [initandlisten] 
2018-02-28T16:39:25.380+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-02-28T16:39:25.380+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-02-28T16:39:25.380+0800 I CONTROL  [initandlisten] 
2018-02-28T16:39:25.380+0800 I CONTROL  [initandlisten] 
2018-02-28T16:39:25.380+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-02-28T16:39:25.403+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-02-28T16:39:25.408+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-02-28T16:39:25.408+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T16:39:25.409+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-02-28T16:39:25.411+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-02-28T16:39:25.411+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-02-28T16:39:25.411+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-02-28T16:39:25.411+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-02-28T16:39:25.411+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-02-28T16:39:25.412+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-02-28T16:39:25.412+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-02-28T16:39:25.412+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-02-28T16:39:25.412+0800 I REPL     [rsSync] transition to RECOVERING
2018-02-28T16:39:25.412+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:39:25.412+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T16:39:25.413+0800 I REPL     [rsSync] transition to SECONDARY
2018-02-28T16:39:25.413+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T16:39:25.413+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-02-28T16:39:25.414+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-02-28T16:39:25.414+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-02-28T16:39:25.563+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:52504 #1 (1 connection now open)
2018-02-28T16:39:25.564+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:52504 conn1: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T16:39:25.776+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:52503 #2 (2 connections now open)
2018-02-28T16:39:25.776+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:52503 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T16:39:30.419+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-02-28T16:39:30.427+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28002
2018-02-28T16:39:30.427+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-02-28T16:39:30.428+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-02-28T16:39:30.429+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-02-28T16:39:30.429+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 0ms (2 connections now open to 127.0.0.1:28002)
2018-02-28T16:39:55.415+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-02-28T16:39:55.415+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-02-28T16:39:55.416+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-02-28T16:39:55.416+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-02-28T16:39:55.417+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-02-28T16:39:55.419+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-02-28T16:39:55.421+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-02-28T16:39:55.423+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-02-28T16:40:14.819+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:52620 #3 (3 connections now open)
2018-02-28T16:40:14.819+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:52620 conn3: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T16:40:16.824+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:52636 #4 (4 connections now open)
2018-02-28T16:40:16.824+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:52636 conn4: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T16:40:19.829+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807214000|2, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3004ms
2018-02-28T16:40:20.065+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807219000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 211ms
2018-02-28T16:41:44.792+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807300000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4477ms
2018-02-28T16:42:09.837+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807324000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4985ms
2018-02-28T16:43:00.535+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807379000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 579ms
2018-02-28T16:45:05.875+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807500000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4997ms
2018-02-28T16:45:20.889+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807515000|3, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4994ms
2018-02-28T16:46:26.130+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807581000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4997ms
2018-02-28T16:46:41.146+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807596000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4988ms
2018-02-28T16:48:56.583+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807731000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4998ms
2018-02-28T16:49:11.600+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807746000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4988ms
2018-02-28T16:51:37.024+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807892000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4979ms
2018-02-28T16:51:50.130+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519807907000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3054ms
2018-02-28T16:51:55.541+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27001)
2018-02-28T16:51:55.541+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27001, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:51:55.541+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27001 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27001 : couldn't connect to server 127.0.0.1:27001, connection attempt failed
2018-02-28T16:52:25.546+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27001, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:52:47.566+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54700 #5 (5 connections now open)
2018-02-28T16:52:47.584+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:54700 conn5: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T16:52:55.580+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-02-28T16:53:17.580+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54776 #6 (6 connections now open)
2018-02-28T16:53:17.580+0800 I NETWORK  [conn6] received client metadata from 127.0.0.1:54776 conn6: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T16:53:25.427+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519808000000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4998ms
2018-02-28T16:53:40.442+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519808015000|1, t: 2 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4994ms
2018-02-28T16:53:52.766+0800 I -        [conn2] end connection 127.0.0.1:52503 (6 connections now open)
2018-02-28T16:53:53.074+0800 I ASIO     [ReplicationExecutor] dropping unhealthy pooled connection to 127.0.0.1:28001
2018-02-28T16:53:53.074+0800 I ASIO     [ReplicationExecutor] after drop, pool was empty, going to spawn some connections
2018-02-28T16:53:53.074+0800 I ASIO     [ReplicationExecutor] Failed to close stream: Socket is not connected
2018-02-28T16:53:53.074+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:53:53.074+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:53:53.074+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:53:53.074+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:53:53.074+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:53:53.074+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:53:53.075+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:53:53.075+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:53:53.075+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:53:53.075+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:53:53.075+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:53:53.075+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:53:55.079+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:53:55.080+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:53:55.080+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:53:55.080+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:53:55.080+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:53:55.080+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:53:55.080+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:53:55.080+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:53:55.080+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:53:55.081+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:53:55.081+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:53:55.081+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:53:55.585+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27002)
2018-02-28T16:53:55.586+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27002, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:53:55.586+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27002 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27002 : couldn't connect to server 127.0.0.1:27002, connection attempt failed
2018-02-28T16:53:55.586+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27003)
2018-02-28T16:53:55.586+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27003, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:53:55.586+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27003 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27003 : couldn't connect to server 127.0.0.1:27003, connection attempt failed
2018-02-28T16:53:55.586+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] No primary detected for set set1
2018-02-28T16:53:55.586+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27004)
2018-02-28T16:53:55.587+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27004, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:53:55.587+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27004 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27004 : couldn't connect to server 127.0.0.1:27004, connection attempt failed
2018-02-28T16:53:55.587+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27005)
2018-02-28T16:53:55.587+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27005, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:53:55.587+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27005 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27005 : couldn't connect to server 127.0.0.1:27005, connection attempt failed
2018-02-28T16:53:55.587+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27006)
2018-02-28T16:53:55.587+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Failed to connect to 127.0.0.1:27006, in(checking socket for error after poll), reason: Connection refused
2018-02-28T16:53:55.587+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Marking host 127.0.0.1:27006 as failed :: caused by :: Location40356: connection pool: connect failed 127.0.0.1:27006 : couldn't connect to server 127.0.0.1:27006, connection attempt failed
2018-02-28T16:53:55.587+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] No primary detected for set set2
2018-02-28T16:53:55.587+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] All nodes for set set2 are down. This has happened for 1 checks in a row.
2018-02-28T16:53:55.667+0800 I REPL     [replication-1] Choosing new sync source because our current sync source, 127.0.0.1:28002, has an OpTime ({ ts: Timestamp 1519808030000|2, t: 2 }) which is not ahead of ours ({ ts: Timestamp 1519808030000|2, t: 2 }), it does not have a sync source, and it's not the primary (sync source does not know the primary)
2018-02-28T16:53:55.667+0800 I REPL     [replication-1] Canceling oplog query due to OplogQueryMetadata. We have to choose a new sync source. Current source: 127.0.0.1:28002, OpTime { ts: Timestamp 1519808030000|2, t: 2 }, its sync source index:-1
2018-02-28T16:53:55.667+0800 W REPL     [rsBackgroundSync] Fetcher stopped querying remote oplog with error: InvalidSyncSource: sync source 127.0.0.1:28002 (config version: 1; last applied optime: { ts: Timestamp 1519808030000|2, t: 2 }; sync source index: -1; primary index: -1) is no longer valid
2018-02-28T16:53:55.667+0800 I REPL     [rsBackgroundSync] could not find member to sync from
2018-02-28T16:53:55.667+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:53:55.667+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:53:55.668+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:53:55.668+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:53:55.668+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:53:55.668+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:53:55.668+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:53:55.668+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:53:55.668+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:53:55.668+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:53:55.668+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:53:55.668+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:53:55.684+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28002: InvalidSyncSource: Sync source was cleared. Was 127.0.0.1:28002
2018-02-28T16:54:00.086+0800 I -        [conn1] end connection 127.0.0.1:52504 (5 connections now open)
2018-02-28T16:54:00.668+0800 I ASIO     [ReplicationExecutor] dropping unhealthy pooled connection to 127.0.0.1:28002
2018-02-28T16:54:00.669+0800 I ASIO     [ReplicationExecutor] after drop, pool was empty, going to spawn some connections
2018-02-28T16:54:00.669+0800 I ASIO     [ReplicationExecutor] Failed to close stream: Socket is not connected
2018-02-28T16:54:00.669+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T16:54:00.669+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:54:00.669+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28002 - HostUnreachable: Connection refused
2018-02-28T16:54:00.669+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28002 due to failed operation on a connection
2018-02-28T16:54:00.669+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28002; HostUnreachable: Connection refused
2018-02-28T16:54:00.669+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:54:00.669+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:54:00.669+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:54:00.669+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T16:54:00.670+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:54:00.670+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28002 - HostUnreachable: Connection refused
2018-02-28T16:54:00.670+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28002 due to failed operation on a connection
2018-02-28T16:54:00.670+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:54:00.670+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:54:00.670+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28002; HostUnreachable: Connection refused
2018-02-28T16:54:00.670+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:54:00.670+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T16:54:00.670+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T16:54:00.671+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28002 - HostUnreachable: Connection refused
2018-02-28T16:54:00.671+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28002 due to failed operation on a connection
2018-02-28T16:54:00.671+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28002; HostUnreachable: Connection refused
2018-02-28T16:54:00.671+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Failed to connect to 127.0.0.1:28001 - HostUnreachable: Connection refused
2018-02-28T16:54:00.671+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Dropping all pooled connections to 127.0.0.1:28001 due to failed operation on a connection
2018-02-28T16:54:00.671+0800 I REPL     [ReplicationExecutor] Error in heartbeat request to 127.0.0.1:28001; HostUnreachable: Connection refused
2018-02-28T16:54:01.182+0800 I REPL     [ReplicationExecutor] Not starting an election, since we are not electable due to: Not standing for election because I cannot see a majority (mask 0x1)
2018-02-28T16:54:03.559+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-02-28T16:54:03.560+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-02-28T16:54:03.560+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-02-28T16:54:03.560+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-02-28T16:54:03.560+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-02-28T16:54:03.560+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-02-28T16:54:03.560+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-02-28T16:54:03.560+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-02-28T16:54:03.560+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-02-28T16:54:03.560+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-02-28T16:54:03.694+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-02-28T16:54:03.694+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-02-28T16:54:03.695+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-02-28T16:54:03.695+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-02-28T16:54:03.698+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-02-28T16:54:03.994+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-02-28T16:54:03.995+0800 I CONTROL  [signalProcessingThread] now exiting
2018-02-28T16:54:03.996+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-02-28T16:54:03.996+0800 I CONTROL  [initandlisten] shutting down with code:0
2018-02-28T17:14:44.810+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-02-28T17:14:44.834+0800 I CONTROL  [initandlisten] MongoDB starting : pid=41522 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-02-28T17:14:44.834+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-02-28T17:14:44.834+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-02-28T17:14:44.834+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-02-28T17:14:44.834+0800 I CONTROL  [initandlisten] allocator: system
2018-02-28T17:14:44.834+0800 I CONTROL  [initandlisten] modules: none
2018-02-28T17:14:44.834+0800 I CONTROL  [initandlisten] build environment:
2018-02-28T17:14:44.834+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-02-28T17:14:44.834+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-02-28T17:14:44.834+0800 I CONTROL  [initandlisten] options: { config: "node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-02-28T17:14:44.836+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-02-28T17:14:44.836+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-02-28T17:14:46.266+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-02-28T17:14:46.266+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 2183 records totaling to 531801 bytes
2018-02-28T17:14:46.266+0800 I STORAGE  [initandlisten] Scanning the oplog to determine where to place markers for truncation
2018-02-28T17:14:46.381+0800 I CONTROL  [initandlisten] 
2018-02-28T17:14:46.381+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-02-28T17:14:46.381+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-02-28T17:14:46.381+0800 I CONTROL  [initandlisten] 
2018-02-28T17:14:46.381+0800 I CONTROL  [initandlisten] 
2018-02-28T17:14:46.381+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-02-28T17:14:46.447+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-02-28T17:14:46.452+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-02-28T17:14:46.452+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T17:14:46.453+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-02-28T17:14:46.456+0800 I REPL     [replExecDBWorker-1] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-02-28T17:14:46.456+0800 I REPL     [replExecDBWorker-1] This node is 127.0.0.1:28003 in the config
2018-02-28T17:14:46.456+0800 I REPL     [replExecDBWorker-1] transition to STARTUP2
2018-02-28T17:14:46.456+0800 I REPL     [replExecDBWorker-1] Starting replication snapshot thread
2018-02-28T17:14:46.456+0800 I REPL     [replExecDBWorker-1] Starting replication storage threads
2018-02-28T17:14:46.456+0800 I REPL     [replExecDBWorker-1] Starting replication fetcher thread
2018-02-28T17:14:46.457+0800 I REPL     [replExecDBWorker-1] Starting replication applier thread
2018-02-28T17:14:46.457+0800 I REPL     [replExecDBWorker-1] Starting replication reporter thread
2018-02-28T17:14:46.457+0800 I REPL     [rsSync] transition to RECOVERING
2018-02-28T17:14:46.457+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T17:14:46.457+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T17:14:46.457+0800 I REPL     [rsSync] transition to SECONDARY
2018-02-28T17:14:46.457+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 0ms (1 connections now open to 127.0.0.1:28002)
2018-02-28T17:14:46.458+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 0ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T17:14:46.458+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-02-28T17:14:46.458+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-02-28T17:14:46.462+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:64071 #1 (1 connection now open)
2018-02-28T17:14:46.462+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:64071 conn1: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:14:46.542+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:64070 #2 (2 connections now open)
2018-02-28T17:14:46.543+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:64070 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:14:47.639+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:64076 #3 (3 connections now open)
2018-02-28T17:14:47.639+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:64076 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:14:47.646+0800 I -        [conn1] end connection 127.0.0.1:64071 (3 connections now open)
2018-02-28T17:14:51.459+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-02-28T17:14:51.465+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28002
2018-02-28T17:14:51.465+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-02-28T17:14:51.466+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-02-28T17:14:51.467+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-02-28T17:14:51.468+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 1ms (2 connections now open to 127.0.0.1:28002)
2018-02-28T17:15:16.457+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-02-28T17:15:16.457+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-02-28T17:15:16.458+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-02-28T17:15:16.458+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-02-28T17:15:16.459+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-02-28T17:15:16.459+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-02-28T17:15:16.460+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-02-28T17:15:16.460+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-02-28T17:15:56.569+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:64229 #4 (3 connections now open)
2018-02-28T17:15:56.569+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:64229 conn4: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:15:56.572+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:64232 #5 (4 connections now open)
2018-02-28T17:15:56.572+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:64232 conn5: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:16:01.515+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809356000|1, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 2936ms
2018-02-28T17:16:06.434+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809361000|2, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4869ms
2018-02-28T17:16:26.603+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809386000|1, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 113ms
2018-02-28T17:16:41.615+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809396000|2, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4989ms
2018-02-28T17:17:46.791+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809461000|1, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5001ms
2018-02-28T17:18:01.802+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809476000|2, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4987ms
2018-02-28T17:18:36.553+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809511000|3, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4639ms
2018-02-28T17:18:51.563+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809526000|1, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4973ms
2018-02-28T17:19:01.985+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809541000|1, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 405ms
2018-02-28T17:22:47.659+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809762000|1, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 5001ms
2018-02-28T17:23:42.770+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519809817000|1, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4982ms
2018-02-28T17:26:48.327+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519810003000|1, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4999ms
2018-02-28T17:27:13.366+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519810028000|1, t: 3 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4979ms
2018-02-28T17:27:52.180+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-02-28T17:27:52.180+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-02-28T17:27:52.180+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-02-28T17:27:52.180+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-02-28T17:27:52.180+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-02-28T17:27:52.180+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-02-28T17:27:52.180+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-02-28T17:27:52.180+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-02-28T17:27:52.181+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28002: CallbackCanceled: Reporter no longer valid
2018-02-28T17:27:52.181+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-02-28T17:27:52.181+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-02-28T17:27:52.181+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28002 due to bad connection status; 1 connections to that host remain open
2018-02-28T17:27:52.181+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-02-28T17:27:52.188+0800 I -        [conn5] end connection 127.0.0.1:64232 (4 connections now open)
2018-02-28T17:27:52.189+0800 I -        [conn4] end connection 127.0.0.1:64229 (3 connections now open)
2018-02-28T17:27:52.189+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-02-28T17:27:52.189+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-02-28T17:27:52.190+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-02-28T17:27:52.190+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-02-28T17:27:52.192+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-02-28T17:27:52.560+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-02-28T17:27:52.565+0800 I CONTROL  [signalProcessingThread] now exiting
2018-02-28T17:27:52.565+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-02-28T17:27:52.565+0800 I CONTROL  [initandlisten] shutting down with code:0
2018-02-28T17:36:12.222+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-02-28T17:36:12.272+0800 I CONTROL  [initandlisten] MongoDB starting : pid=42817 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-02-28T17:36:12.273+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-02-28T17:36:12.273+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-02-28T17:36:12.273+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-02-28T17:36:12.273+0800 I CONTROL  [initandlisten] allocator: system
2018-02-28T17:36:12.273+0800 I CONTROL  [initandlisten] modules: none
2018-02-28T17:36:12.273+0800 I CONTROL  [initandlisten] build environment:
2018-02-28T17:36:12.273+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-02-28T17:36:12.273+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-02-28T17:36:12.273+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-02-28T17:36:12.283+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-02-28T17:36:12.284+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-02-28T17:36:13.500+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-02-28T17:36:13.500+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 2383 records totaling to 581468 bytes
2018-02-28T17:36:13.513+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Feb 28 17:27:43:2 to determine where to place markers for truncation
2018-02-28T17:36:13.513+0800 I STORAGE  [initandlisten] Taking 0 samples and assuming that each section of oplog contains approximately 75008 records totaling to 18302455 bytes
2018-02-28T17:36:13.627+0800 I CONTROL  [initandlisten] 
2018-02-28T17:36:13.627+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-02-28T17:36:13.627+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-02-28T17:36:13.628+0800 I CONTROL  [initandlisten] 
2018-02-28T17:36:13.628+0800 I CONTROL  [initandlisten] 
2018-02-28T17:36:13.628+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-02-28T17:36:13.708+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-02-28T17:36:13.713+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-02-28T17:36:13.713+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T17:36:13.714+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-02-28T17:36:13.716+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-02-28T17:36:13.716+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-02-28T17:36:13.716+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-02-28T17:36:13.716+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-02-28T17:36:13.716+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-02-28T17:36:13.787+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-02-28T17:36:13.787+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-02-28T17:36:13.787+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-02-28T17:36:13.787+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T17:36:13.787+0800 I REPL     [rsSync] transition to RECOVERING
2018-02-28T17:36:13.788+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T17:36:13.788+0800 I REPL     [rsSync] transition to SECONDARY
2018-02-28T17:36:13.788+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-02-28T17:36:13.788+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T17:36:13.789+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-02-28T17:36:13.789+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-02-28T17:36:15.654+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50227 #1 (1 connection now open)
2018-02-28T17:36:15.654+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:50227 conn1: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:36:17.202+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50232 #2 (2 connections now open)
2018-02-28T17:36:17.202+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:50232 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:36:21.225+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50244 #3 (3 connections now open)
2018-02-28T17:36:21.225+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:50244 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:36:21.231+0800 I -        [conn1] end connection 127.0.0.1:50227 (3 connections now open)
2018-02-28T17:36:23.798+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-02-28T17:36:23.808+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28001
2018-02-28T17:36:23.809+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T17:36:23.809+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T17:36:23.810+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T17:36:23.810+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 0ms (2 connections now open to 127.0.0.1:28001)
2018-02-28T17:36:27.235+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50263 #4 (3 connections now open)
2018-02-28T17:36:27.236+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:50263 conn4: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:36:27.237+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50264 #5 (4 connections now open)
2018-02-28T17:36:27.238+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:50264 conn5: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:36:43.717+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-02-28T17:36:43.717+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-02-28T17:36:43.718+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-02-28T17:36:43.718+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-02-28T17:36:43.719+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-02-28T17:36:43.719+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-02-28T17:36:43.720+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-02-28T17:36:43.720+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-02-28T17:37:38.017+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-02-28T17:37:38.017+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-02-28T17:37:38.017+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-02-28T17:37:38.017+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-02-28T17:37:38.017+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-02-28T17:37:38.017+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-02-28T17:37:38.018+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-02-28T17:37:38.018+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-02-28T17:37:38.018+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28001: CallbackCanceled: Reporter no longer valid
2018-02-28T17:37:38.018+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-02-28T17:37:38.018+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-02-28T17:37:38.018+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28001 due to bad connection status; 1 connections to that host remain open
2018-02-28T17:37:38.018+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-02-28T17:37:38.023+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-02-28T17:37:38.023+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-02-28T17:37:38.028+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-02-28T17:37:38.028+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-02-28T17:37:38.046+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-02-28T17:37:38.605+0800 I -        [conn4] end connection 127.0.0.1:50263 (4 connections now open)
2018-02-28T17:37:38.605+0800 I -        [conn2] end connection 127.0.0.1:50232 (3 connections now open)
2018-02-28T17:37:38.648+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-02-28T17:37:38.648+0800 I CONTROL  [signalProcessingThread] now exiting
2018-02-28T17:37:38.648+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-02-28T17:37:57.727+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-02-28T17:37:57.754+0800 I CONTROL  [initandlisten] MongoDB starting : pid=42964 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-02-28T17:37:57.754+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-02-28T17:37:57.754+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-02-28T17:37:57.754+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-02-28T17:37:57.754+0800 I CONTROL  [initandlisten] allocator: system
2018-02-28T17:37:57.754+0800 I CONTROL  [initandlisten] modules: none
2018-02-28T17:37:57.754+0800 I CONTROL  [initandlisten] build environment:
2018-02-28T17:37:57.754+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-02-28T17:37:57.754+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-02-28T17:37:57.754+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-02-28T17:37:57.756+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-02-28T17:37:57.756+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-02-28T17:37:59.240+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-02-28T17:37:59.241+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 2397 records totaling to 585419 bytes
2018-02-28T17:37:59.252+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Feb 28 17:37:34:1 to determine where to place markers for truncation
2018-02-28T17:37:59.252+0800 I STORAGE  [initandlisten] Taking 0 samples and assuming that each section of oplog contains approximately 74940 records totaling to 18302586 bytes
2018-02-28T17:37:59.355+0800 I CONTROL  [initandlisten] 
2018-02-28T17:37:59.355+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-02-28T17:37:59.355+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-02-28T17:37:59.355+0800 I CONTROL  [initandlisten] 
2018-02-28T17:37:59.355+0800 I CONTROL  [initandlisten] 
2018-02-28T17:37:59.355+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-02-28T17:37:59.439+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-02-28T17:37:59.444+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-02-28T17:37:59.444+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T17:37:59.445+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-02-28T17:37:59.446+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-02-28T17:37:59.446+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-02-28T17:37:59.446+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-02-28T17:37:59.447+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-02-28T17:37:59.447+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-02-28T17:37:59.447+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-02-28T17:37:59.447+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-02-28T17:37:59.447+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-02-28T17:37:59.447+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T17:37:59.447+0800 I REPL     [rsSync] transition to RECOVERING
2018-02-28T17:37:59.447+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T17:37:59.448+0800 I REPL     [rsSync] transition to SECONDARY
2018-02-28T17:37:59.448+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T17:37:59.448+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-02-28T17:37:59.448+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-02-28T17:37:59.448+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-02-28T17:38:00.982+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50507 #1 (1 connection now open)
2018-02-28T17:38:00.983+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:50507 conn1: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:38:02.707+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50518 #2 (2 connections now open)
2018-02-28T17:38:02.707+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:50518 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:38:06.501+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50529 #3 (3 connections now open)
2018-02-28T17:38:06.501+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:50529 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:38:06.508+0800 I -        [conn1] end connection 127.0.0.1:50507 (3 connections now open)
2018-02-28T17:38:09.452+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-02-28T17:38:09.480+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28001
2018-02-28T17:38:09.480+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T17:38:09.481+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T17:38:09.482+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T17:38:09.482+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 0ms (2 connections now open to 127.0.0.1:28001)
2018-02-28T17:38:12.757+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50550 #4 (3 connections now open)
2018-02-28T17:38:12.757+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:50550 conn4: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:38:12.758+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50551 #5 (4 connections now open)
2018-02-28T17:38:12.759+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:50551 conn5: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:38:29.449+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-02-28T17:38:29.450+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-02-28T17:38:29.450+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-02-28T17:38:29.451+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-02-28T17:38:29.452+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-02-28T17:38:29.453+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-02-28T17:38:29.454+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-02-28T17:38:29.454+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-02-28T17:39:58.472+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-02-28T17:39:58.472+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-02-28T17:39:58.472+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-02-28T17:39:58.472+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-02-28T17:39:58.472+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-02-28T17:39:58.472+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-02-28T17:39:58.472+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-02-28T17:39:58.472+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-02-28T17:39:58.472+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28001: CallbackCanceled: Reporter no longer valid
2018-02-28T17:39:58.472+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-02-28T17:39:58.472+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-02-28T17:39:58.474+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28001 due to bad connection status; 1 connections to that host remain open
2018-02-28T17:39:58.474+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-02-28T17:39:58.476+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-02-28T17:39:58.476+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-02-28T17:39:58.478+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-02-28T17:39:58.478+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-02-28T17:39:58.481+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-02-28T17:39:59.412+0800 I -        [conn4] end connection 127.0.0.1:50550 (4 connections now open)
2018-02-28T17:39:59.413+0800 I -        [conn2] end connection 127.0.0.1:50518 (3 connections now open)
2018-02-28T17:39:59.457+0800 I NETWORK  [PeriodicTaskRunner] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27002)
2018-02-28T17:39:59.457+0800 I NETWORK  [PeriodicTaskRunner] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27003)
2018-02-28T17:39:59.457+0800 I NETWORK  [PeriodicTaskRunner] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27005)
2018-02-28T17:39:59.457+0800 I NETWORK  [PeriodicTaskRunner] Socket closed remotely, no longer connected (idle 30 secs, remote host 127.0.0.1:27006)
2018-02-28T17:39:59.467+0800 W SHARDING [replSetDistLockPinger] pinging failed for distributed lock pinger :: caused by :: InterruptedAtShutdown: interrupted at shutdown
2018-02-28T17:39:59.667+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-02-28T17:39:59.667+0800 I CONTROL  [signalProcessingThread] now exiting
2018-02-28T17:39:59.667+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-02-28T17:39:59.667+0800 I CONTROL  [initandlisten] shutting down with code:0
2018-02-28T17:40:03.852+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-02-28T17:40:03.877+0800 I CONTROL  [initandlisten] MongoDB starting : pid=43127 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-02-28T17:40:03.877+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-02-28T17:40:03.877+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-02-28T17:40:03.877+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-02-28T17:40:03.877+0800 I CONTROL  [initandlisten] allocator: system
2018-02-28T17:40:03.877+0800 I CONTROL  [initandlisten] modules: none
2018-02-28T17:40:03.877+0800 I CONTROL  [initandlisten] build environment:
2018-02-28T17:40:03.877+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-02-28T17:40:03.877+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-02-28T17:40:03.877+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-02-28T17:40:03.878+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-02-28T17:40:03.878+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-02-28T17:40:05.493+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-02-28T17:40:05.493+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 2416 records totaling to 590812 bytes
2018-02-28T17:40:05.493+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Feb 28 17:39:56:1 to determine where to place markers for truncation
2018-02-28T17:40:05.493+0800 I STORAGE  [initandlisten] Taking 0 samples and assuming that each section of oplog contains approximately 74844 records totaling to 18302455 bytes
2018-02-28T17:40:05.523+0800 I CONTROL  [initandlisten] 
2018-02-28T17:40:05.523+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-02-28T17:40:05.523+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-02-28T17:40:05.523+0800 I CONTROL  [initandlisten] 
2018-02-28T17:40:05.523+0800 I CONTROL  [initandlisten] 
2018-02-28T17:40:05.523+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-02-28T17:40:05.531+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-02-28T17:40:05.537+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-02-28T17:40:05.537+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T17:40:05.537+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-02-28T17:40:05.539+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-02-28T17:40:05.539+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-02-28T17:40:05.539+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-02-28T17:40:05.540+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-02-28T17:40:05.540+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-02-28T17:40:05.540+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-02-28T17:40:05.540+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-02-28T17:40:05.540+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-02-28T17:40:05.540+0800 I REPL     [rsSync] transition to RECOVERING
2018-02-28T17:40:05.540+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T17:40:05.540+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T17:40:05.541+0800 I REPL     [rsSync] transition to SECONDARY
2018-02-28T17:40:05.541+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T17:40:05.541+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-02-28T17:40:05.541+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-02-28T17:40:05.542+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-02-28T17:40:06.088+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50807 #1 (1 connection now open)
2018-02-28T17:40:06.088+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:50807 conn1: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:40:07.012+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50813 #2 (2 connections now open)
2018-02-28T17:40:07.012+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:50813 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:40:08.841+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50819 #3 (3 connections now open)
2018-02-28T17:40:08.841+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:50819 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:40:13.360+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50836 #4 (4 connections now open)
2018-02-28T17:40:13.361+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:50836 conn4: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:40:13.371+0800 I -        [conn2] end connection 127.0.0.1:50813 (4 connections now open)
2018-02-28T17:40:15.546+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-02-28T17:40:15.577+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28001
2018-02-28T17:40:15.577+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T17:40:15.578+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T17:40:15.579+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T17:40:15.579+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 0ms (2 connections now open to 127.0.0.1:28001)
2018-02-28T17:40:15.728+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-02-28T17:40:15.729+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-02-28T17:40:15.729+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-02-28T17:40:15.729+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-02-28T17:40:15.729+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-02-28T17:40:15.729+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-02-28T17:40:15.730+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-02-28T17:40:15.730+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-02-28T17:40:15.730+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28001: CallbackCanceled: Reporter no longer valid
2018-02-28T17:40:15.730+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-02-28T17:40:15.730+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-02-28T17:40:15.730+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28001 due to bad connection status; 1 connections to that host remain open
2018-02-28T17:40:15.731+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-02-28T17:40:15.732+0800 I -        [conn1] end connection 127.0.0.1:50807 (3 connections now open)
2018-02-28T17:40:15.732+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-02-28T17:40:15.732+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-02-28T17:40:15.733+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-02-28T17:40:15.733+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-02-28T17:40:15.734+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-02-28T17:40:16.069+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-02-28T17:40:16.070+0800 I CONTROL  [signalProcessingThread] now exiting
2018-02-28T17:40:16.070+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-02-28T17:40:16.070+0800 I CONTROL  [initandlisten] shutting down with code:0
2018-02-28T17:40:49.125+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-02-28T17:40:49.152+0800 I CONTROL  [initandlisten] MongoDB starting : pid=43211 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-02-28T17:40:49.152+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-02-28T17:40:49.152+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-02-28T17:40:49.152+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-02-28T17:40:49.152+0800 I CONTROL  [initandlisten] allocator: system
2018-02-28T17:40:49.152+0800 I CONTROL  [initandlisten] modules: none
2018-02-28T17:40:49.152+0800 I CONTROL  [initandlisten] build environment:
2018-02-28T17:40:49.152+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-02-28T17:40:49.152+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-02-28T17:40:49.152+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-02-28T17:40:49.153+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-02-28T17:40:49.153+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-02-28T17:40:50.842+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-02-28T17:40:50.842+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 2421 records totaling to 591563 bytes
2018-02-28T17:40:50.842+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Feb 28 17:40:15:5 to determine where to place markers for truncation
2018-02-28T17:40:50.842+0800 I STORAGE  [initandlisten] Taking 0 samples and assuming that each section of oplog contains approximately 74904 records totaling to 18302534 bytes
2018-02-28T17:40:50.871+0800 I CONTROL  [initandlisten] 
2018-02-28T17:40:50.871+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-02-28T17:40:50.871+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-02-28T17:40:50.871+0800 I CONTROL  [initandlisten] 
2018-02-28T17:40:50.871+0800 I CONTROL  [initandlisten] 
2018-02-28T17:40:50.872+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-02-28T17:40:50.879+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-02-28T17:40:50.885+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-02-28T17:40:50.885+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-02-28T17:40:50.885+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-02-28T17:40:50.887+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-02-28T17:40:50.888+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-02-28T17:40:50.888+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-02-28T17:40:50.888+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-02-28T17:40:50.888+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-02-28T17:40:50.888+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-02-28T17:40:50.888+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-02-28T17:40:50.888+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-02-28T17:40:50.888+0800 I REPL     [rsSync] transition to RECOVERING
2018-02-28T17:40:50.888+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-02-28T17:40:50.889+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-02-28T17:40:50.889+0800 I REPL     [rsSync] transition to SECONDARY
2018-02-28T17:40:50.889+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T17:40:50.889+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-02-28T17:40:50.890+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-02-28T17:40:50.890+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-02-28T17:40:50.925+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50987 #1 (1 connection now open)
2018-02-28T17:40:50.925+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:50987 conn1: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:40:50.927+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50989 #2 (2 connections now open)
2018-02-28T17:40:50.927+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:50989 conn2: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:40:52.174+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50995 #3 (3 connections now open)
2018-02-28T17:40:52.174+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:50995 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:40:54.111+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51003 #4 (4 connections now open)
2018-02-28T17:40:54.111+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:51003 conn4: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:40:57.738+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51010 #5 (5 connections now open)
2018-02-28T17:40:57.738+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:51010 conn5: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:40:57.744+0800 I -        [conn3] end connection 127.0.0.1:50995 (5 connections now open)
2018-02-28T17:40:59.135+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51017 #6 (5 connections now open)
2018-02-28T17:40:59.135+0800 I NETWORK  [conn6] received client metadata from 127.0.0.1:51017 conn6: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:40:59.136+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51018 #7 (6 connections now open)
2018-02-28T17:40:59.136+0800 I NETWORK  [conn7] received client metadata from 127.0.0.1:51018 conn7: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-02-28T17:41:00.896+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-02-28T17:41:00.917+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28001
2018-02-28T17:41:00.917+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T17:41:00.917+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 0ms (1 connections now open to 127.0.0.1:28001)
2018-02-28T17:41:00.918+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T17:41:00.919+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (2 connections now open to 127.0.0.1:28001)
2018-02-28T17:41:00.931+0800 I COMMAND  [conn2] command config.shards command: find { find: "shards", readConcern: { level: "majority", afterOpTime: { ts: Timestamp 0|0, t: -1 } }, maxTimeMS: 30000 } planSummary: COLLSCAN keysExamined:0 docsExamined:2 cursorExhausted:1 numYields:0 nreturned:2 reslen:544 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 10002ms
2018-02-28T17:41:20.891+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-02-28T17:41:20.891+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-02-28T17:41:20.892+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-02-28T17:41:20.892+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-02-28T17:41:20.893+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-02-28T17:41:20.894+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-02-28T17:41:20.894+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-02-28T17:41:20.895+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-02-28T19:07:57.337+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T19:07:57.339+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 2ms (3 connections now open to 127.0.0.1:28001)
2018-02-28T19:08:57.349+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending idle connection to host 127.0.0.1:28001 because the pool meets constraints; 2 connections to that host remain open
2018-02-28T19:13:18.289+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T19:13:18.294+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 4ms (3 connections now open to 127.0.0.1:28001)
2018-02-28T19:14:18.298+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending idle connection to host 127.0.0.1:28001 because the pool meets constraints; 2 connections to that host remain open
2018-02-28T23:07:10.360+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-02-28T23:07:10.362+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 3ms (3 connections now open to 127.0.0.1:28001)
2018-02-28T23:08:10.370+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending idle connection to host 127.0.0.1:28001 because the pool meets constraints; 2 connections to that host remain open
2018-03-01T03:43:05.548+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T03:43:05.551+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 3ms (3 connections now open to 127.0.0.1:28001)
2018-03-01T03:44:05.559+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending idle connection to host 127.0.0.1:28001 because the pool meets constraints; 2 connections to that host remain open
2018-03-01T03:54:37.893+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T03:54:37.896+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 3ms (3 connections now open to 127.0.0.1:28001)
2018-03-01T03:55:37.897+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending idle connection to host 127.0.0.1:28001 because the pool meets constraints; 2 connections to that host remain open
2018-03-01T06:58:32.608+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T06:58:32.609+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (3 connections now open to 127.0.0.1:28001)
2018-03-01T06:59:32.613+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending idle connection to host 127.0.0.1:28001 because the pool meets constraints; 2 connections to that host remain open
2018-03-01T10:04:41.509+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-03-01T10:04:41.509+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-03-01T10:04:41.509+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-03-01T10:04:41.510+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-03-01T10:04:41.510+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-03-01T10:04:41.511+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-03-01T10:04:41.511+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-03-01T10:04:41.511+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-03-01T10:04:41.512+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28001: CallbackCanceled: Reporter no longer valid
2018-03-01T10:04:41.512+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-03-01T10:04:41.512+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-03-01T10:04:41.512+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28001 due to bad connection status; 1 connections to that host remain open
2018-03-01T10:04:41.512+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-03-01T10:04:41.519+0800 I -        [conn2] end connection 127.0.0.1:50989 (6 connections now open)
2018-03-01T10:04:41.519+0800 I -        [conn1] end connection 127.0.0.1:50987 (5 connections now open)
2018-03-01T10:04:41.520+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-03-01T10:04:41.520+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-03-01T10:04:41.521+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-03-01T10:04:41.521+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-03-01T10:04:41.527+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-03-01T10:04:42.036+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-03-01T10:04:42.041+0800 I CONTROL  [signalProcessingThread] now exiting
2018-03-01T10:04:42.041+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-03-01T10:04:42.041+0800 I CONTROL  [initandlisten] shutting down with code:0
2018-03-01T10:20:35.554+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-03-01T10:20:35.576+0800 I CONTROL  [initandlisten] MongoDB starting : pid=749 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-03-01T10:20:35.576+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-03-01T10:20:35.576+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-03-01T10:20:35.576+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-03-01T10:20:35.576+0800 I CONTROL  [initandlisten] allocator: system
2018-03-01T10:20:35.576+0800 I CONTROL  [initandlisten] modules: none
2018-03-01T10:20:35.576+0800 I CONTROL  [initandlisten] build environment:
2018-03-01T10:20:35.576+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-03-01T10:20:35.576+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-03-01T10:20:35.576+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-03-01T10:20:35.577+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-03-01T10:20:35.577+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-03-01T10:20:37.068+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-03-01T10:20:37.068+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 17523 records totaling to 4339605 bytes
2018-03-01T10:20:37.070+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Mar  1 10:04:40:1 to determine where to place markers for truncation
2018-03-01T10:20:37.070+0800 I STORAGE  [initandlisten] Taking 2 samples and assuming that each section of oplog contains approximately 73904 records totaling to 18302469 bytes
2018-03-01T10:20:37.117+0800 I CONTROL  [initandlisten] 
2018-03-01T10:20:37.117+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-03-01T10:20:37.117+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-03-01T10:20:37.117+0800 I CONTROL  [initandlisten] 
2018-03-01T10:20:37.117+0800 I CONTROL  [initandlisten] 
2018-03-01T10:20:37.117+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-03-01T10:20:37.137+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-03-01T10:20:37.143+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-03-01T10:20:37.143+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-03-01T10:20:37.144+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-03-01T10:20:37.146+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-03-01T10:20:37.146+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-03-01T10:20:37.146+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-03-01T10:20:37.146+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-03-01T10:20:37.146+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-03-01T10:20:37.147+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-03-01T10:20:37.147+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-03-01T10:20:37.147+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-03-01T10:20:37.147+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-03-01T10:20:37.147+0800 I REPL     [rsSync] transition to RECOVERING
2018-03-01T10:20:37.147+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-03-01T10:20:37.147+0800 I REPL     [rsSync] transition to SECONDARY
2018-03-01T10:20:37.148+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T10:20:37.148+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-03-01T10:20:37.148+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-03-01T10:20:37.148+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-03-01T10:20:37.402+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:62768 #1 (1 connection now open)
2018-03-01T10:20:37.402+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:62768 conn1: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T10:20:38.523+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:62775 #2 (2 connections now open)
2018-03-01T10:20:38.523+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:62775 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T10:20:40.320+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:62780 #3 (3 connections now open)
2018-03-01T10:20:40.321+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:62780 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T10:20:47.154+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-03-01T10:20:47.183+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28001
2018-03-01T10:20:47.184+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T10:20:47.184+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 0ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T10:20:47.185+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T10:20:47.186+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (2 connections now open to 127.0.0.1:28001)
2018-03-01T10:20:50.382+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:62815 #4 (4 connections now open)
2018-03-01T10:20:50.384+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:62815 conn4: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T10:20:50.385+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:62816 #5 (5 connections now open)
2018-03-01T10:20:50.385+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:62816 conn5: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T10:20:52.387+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:62827 #6 (6 connections now open)
2018-03-01T10:20:52.387+0800 I NETWORK  [conn6] received client metadata from 127.0.0.1:62827 conn6: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T10:21:07.148+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-03-01T10:21:07.148+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-03-01T10:21:07.148+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-03-01T10:21:07.149+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-03-01T10:21:07.150+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-03-01T10:21:07.150+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-03-01T10:21:07.151+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-03-01T10:21:07.151+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-03-01T10:21:23.103+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-03-01T10:21:23.103+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-03-01T10:21:23.103+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-03-01T10:21:23.104+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-03-01T10:21:23.104+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-03-01T10:21:23.105+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-03-01T10:21:23.105+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-03-01T10:21:23.105+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-03-01T10:21:23.105+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28001: CallbackCanceled: Reporter no longer valid
2018-03-01T10:21:23.107+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-03-01T10:21:23.107+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-03-01T10:21:23.107+0800 I -        [conn6] end connection 127.0.0.1:62827 (6 connections now open)
2018-03-01T10:21:23.107+0800 I -        [conn1] end connection 127.0.0.1:62768 (5 connections now open)
2018-03-01T10:21:23.107+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28001 due to bad connection status; 1 connections to that host remain open
2018-03-01T10:21:23.107+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-03-01T10:21:23.110+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-03-01T10:21:23.110+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-03-01T10:21:23.111+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-03-01T10:21:23.111+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-03-01T10:21:23.138+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-03-01T10:21:24.168+0800 I -        [conn4] end connection 127.0.0.1:62815 (4 connections now open)
2018-03-01T10:21:24.168+0800 I -        [conn3] end connection 127.0.0.1:62780 (3 connections now open)
2018-03-01T10:21:24.350+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-03-01T10:21:24.350+0800 I CONTROL  [signalProcessingThread] now exiting
2018-03-01T10:21:24.350+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-03-01T10:21:24.350+0800 I CONTROL  [initandlisten] shutting down with code:0
2018-03-01T10:25:18.516+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-03-01T10:25:18.576+0800 I CONTROL  [initandlisten] MongoDB starting : pid=1071 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-03-01T10:25:18.576+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-03-01T10:25:18.576+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-03-01T10:25:18.576+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-03-01T10:25:18.576+0800 I CONTROL  [initandlisten] allocator: system
2018-03-01T10:25:18.576+0800 I CONTROL  [initandlisten] modules: none
2018-03-01T10:25:18.576+0800 I CONTROL  [initandlisten] build environment:
2018-03-01T10:25:18.576+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-03-01T10:25:18.576+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-03-01T10:25:18.576+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-03-01T10:25:18.585+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-03-01T10:25:18.585+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-03-01T10:25:20.120+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-03-01T10:25:20.120+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 17537 records totaling to 4342699 bytes
2018-03-01T10:25:20.121+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Mar  1 10:21:17:3 to determine where to place markers for truncation
2018-03-01T10:25:20.121+0800 I STORAGE  [initandlisten] Taking 2 samples and assuming that each section of oplog contains approximately 73911 records totaling to 18302630 bytes
2018-03-01T10:25:20.266+0800 I CONTROL  [initandlisten] 
2018-03-01T10:25:20.266+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-03-01T10:25:20.266+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-03-01T10:25:20.266+0800 I CONTROL  [initandlisten] 
2018-03-01T10:25:20.266+0800 I CONTROL  [initandlisten] 
2018-03-01T10:25:20.266+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-03-01T10:25:20.318+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-03-01T10:25:20.324+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-03-01T10:25:20.324+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-03-01T10:25:20.325+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-03-01T10:25:20.327+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-03-01T10:25:20.327+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-03-01T10:25:20.327+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-03-01T10:25:20.327+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-03-01T10:25:20.327+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-03-01T10:25:20.328+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-03-01T10:25:20.328+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-03-01T10:25:20.328+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-03-01T10:25:20.328+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-03-01T10:25:20.328+0800 I REPL     [rsSync] transition to RECOVERING
2018-03-01T10:25:20.328+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-03-01T10:25:20.328+0800 I REPL     [rsSync] transition to SECONDARY
2018-03-01T10:25:20.329+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T10:25:20.329+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-03-01T10:25:20.330+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-03-01T10:25:20.330+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-03-01T10:25:20.519+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:63387 #1 (1 connection now open)
2018-03-01T10:25:20.519+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:63387 conn1: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T10:25:20.634+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:63396 #2 (2 connections now open)
2018-03-01T10:25:20.634+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:63396 conn2: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T10:25:21.538+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:63403 #3 (3 connections now open)
2018-03-01T10:25:21.538+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:63403 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T10:25:28.696+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:63438 #4 (4 connections now open)
2018-03-01T10:25:28.696+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:63438 conn4: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T10:25:30.334+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-03-01T10:25:30.364+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28002
2018-03-01T10:25:30.365+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-03-01T10:25:30.365+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 0ms (1 connections now open to 127.0.0.1:28002)
2018-03-01T10:25:30.366+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-03-01T10:25:30.366+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 0ms (2 connections now open to 127.0.0.1:28002)
2018-03-01T10:25:30.373+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "autosplit" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519871128000|5, t: 9 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 1676ms
2018-03-01T10:25:45.385+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "chunksize" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519871140000|1, t: 9 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 4987ms
2018-03-01T10:25:50.326+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-03-01T10:25:50.326+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-03-01T10:25:50.327+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-03-01T10:25:50.327+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-03-01T10:25:50.328+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-03-01T10:25:50.328+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-03-01T10:25:50.329+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-03-01T10:25:50.329+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-03-01T10:25:58.748+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519871155000|1, t: 9 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 3343ms
2018-03-01T10:25:59.871+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-03-01T10:25:59.871+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-03-01T10:25:59.871+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-03-01T10:25:59.872+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-03-01T10:25:59.873+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-03-01T10:25:59.873+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-03-01T10:25:59.873+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-03-01T10:25:59.873+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-03-01T10:25:59.874+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28002: CallbackCanceled: Reporter no longer valid
2018-03-01T10:25:59.874+0800 I -        [conn4] end connection 127.0.0.1:63438 (4 connections now open)
2018-03-01T10:25:59.874+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-03-01T10:25:59.874+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-03-01T10:25:59.875+0800 I -        [conn2] end connection 127.0.0.1:63396 (3 connections now open)
2018-03-01T10:25:59.875+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28002 due to bad connection status; 1 connections to that host remain open
2018-03-01T10:25:59.875+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-03-01T10:25:59.877+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-03-01T10:25:59.877+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-03-01T10:25:59.879+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-03-01T10:25:59.879+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-03-01T10:25:59.881+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-03-01T10:26:01.190+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-03-01T10:26:01.190+0800 I CONTROL  [signalProcessingThread] now exiting
2018-03-01T10:26:01.190+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-03-01T10:26:01.190+0800 I CONTROL  [initandlisten] shutting down with code:0
2018-03-01T10:50:24.927+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-03-01T10:50:24.994+0800 I CONTROL  [initandlisten] MongoDB starting : pid=1577 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-03-01T10:50:24.994+0800 I CONTROL  [initandlisten] db version v3.2.19-15-g88a5532
2018-03-01T10:50:24.994+0800 I CONTROL  [initandlisten] git version: 88a553287ab648d66e5ec96b328c6c1bbd88cc34
2018-03-01T10:50:24.994+0800 I CONTROL  [initandlisten] allocator: system
2018-03-01T10:50:24.994+0800 I CONTROL  [initandlisten] modules: none
2018-03-01T10:50:24.994+0800 I CONTROL  [initandlisten] build environment:
2018-03-01T10:50:24.994+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-03-01T10:50:24.994+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-03-01T10:50:24.994+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-03-01T10:50:24.995+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-03-01T10:50:24.995+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=4G,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),verbose=(recovery_progress),
2018-03-01T10:50:25.563+0800 I STORAGE  [initandlisten] WiredTiger [1519872625:563581][1577:0x7fff72c12000], txn-recover: Main recovery loop: starting at 9/32000
2018-03-01T10:50:25.614+0800 I STORAGE  [initandlisten] WiredTiger [1519872625:614720][1577:0x7fff72c12000], txn-recover: Recovering log 9 through 10
2018-03-01T10:50:25.625+0800 I STORAGE  [initandlisten] WiredTiger [1519872625:625905][1577:0x7fff72c12000], txn-recover: Recovering log 10 through 10
2018-03-01T10:50:26.348+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-03-01T10:50:26.349+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 17551 records totaling to 4345793 bytes
2018-03-01T10:50:26.349+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Mar  1 10:25:58:2 to determine where to place markers for truncation
2018-03-01T10:50:26.349+0800 I STORAGE  [initandlisten] Taking 2 samples and assuming that each section of oplog contains approximately 73917 records totaling to 18302545 bytes
2018-03-01T10:50:26.485+0800 I CONTROL  [initandlisten] 
2018-03-01T10:50:26.485+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-03-01T10:50:26.486+0800 F INDEX    [initandlisten] Found an invalid index { v: 2, key: { _id: 1 }, name: "_id_", ns: "local.replset.election" } on the local.replset.election collection: this version of mongod cannot build new indexes of version number 2
2018-03-01T10:50:26.486+0800 I -        [initandlisten] Fatal Assertion 28782
2018-03-01T10:50:26.486+0800 I -        [initandlisten] 

***aborting after fassert() failure


2018-03-01T10:51:55.662+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-03-01T10:51:55.689+0800 I CONTROL  [initandlisten] MongoDB starting : pid=1699 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-03-01T10:51:55.689+0800 I CONTROL  [initandlisten] db version v3.2.19-15-g88a5532
2018-03-01T10:51:55.689+0800 I CONTROL  [initandlisten] git version: 88a553287ab648d66e5ec96b328c6c1bbd88cc34
2018-03-01T10:51:55.689+0800 I CONTROL  [initandlisten] allocator: system
2018-03-01T10:51:55.689+0800 I CONTROL  [initandlisten] modules: none
2018-03-01T10:51:55.689+0800 I CONTROL  [initandlisten] build environment:
2018-03-01T10:51:55.689+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-03-01T10:51:55.689+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-03-01T10:51:55.689+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-03-01T10:51:55.690+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-03-01T10:51:55.690+0800 W -        [initandlisten] Detected unclean shutdown - /usr/local/mongo-cluster-sharding/configsvr/node3/data/db/mongod.lock is not empty.
2018-03-01T10:51:55.691+0800 W STORAGE  [initandlisten] Recovering data from the last clean checkpoint.
2018-03-01T10:51:55.691+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=4G,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),verbose=(recovery_progress),
2018-03-01T10:51:57.547+0800 I STORAGE  [initandlisten] WiredTiger [1519872717:547389][1699:0x7fff72c12000], txn-recover: Main recovery loop: starting at 10/128
2018-03-01T10:51:57.547+0800 I STORAGE  [initandlisten] WiredTiger [1519872717:547728][1699:0x7fff72c12000], txn-recover: Recovering log 10 through 11
2018-03-01T10:51:57.773+0800 I STORAGE  [initandlisten] WiredTiger [1519872717:773330][1699:0x7fff72c12000], txn-recover: Recovering log 11 through 11
2018-03-01T10:52:00.913+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-03-01T10:52:00.913+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 17551 records totaling to 4345793 bytes
2018-03-01T10:52:00.920+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Mar  1 10:25:58:2 to determine where to place markers for truncation
2018-03-01T10:52:00.920+0800 I STORAGE  [initandlisten] Taking 2 samples and assuming that each section of oplog contains approximately 73917 records totaling to 18302545 bytes
2018-03-01T10:52:01.267+0800 I CONTROL  [initandlisten] 
2018-03-01T10:52:01.267+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-03-01T10:52:01.268+0800 F INDEX    [initandlisten] Found an invalid index { v: 2, key: { _id: 1 }, name: "_id_", ns: "local.replset.election" } on the local.replset.election collection: this version of mongod cannot build new indexes of version number 2
2018-03-01T10:52:01.268+0800 I -        [initandlisten] Fatal Assertion 28782
2018-03-01T10:52:01.268+0800 I -        [initandlisten] 

***aborting after fassert() failure


2018-03-01T11:08:50.892+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-03-01T11:08:50.925+0800 I CONTROL  [initandlisten] MongoDB starting : pid=1317 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-03-01T11:08:50.925+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-03-01T11:08:50.925+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-03-01T11:08:50.925+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-03-01T11:08:50.925+0800 I CONTROL  [initandlisten] allocator: system
2018-03-01T11:08:50.925+0800 I CONTROL  [initandlisten] modules: none
2018-03-01T11:08:50.925+0800 I CONTROL  [initandlisten] build environment:
2018-03-01T11:08:50.925+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-03-01T11:08:50.925+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-03-01T11:08:50.925+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-03-01T11:08:50.930+0800 W -        [initandlisten] Detected unclean shutdown - /usr/local/mongo-cluster-sharding/configsvr/node3/data/db/mongod.lock is not empty.
2018-03-01T11:08:50.950+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-03-01T11:08:50.950+0800 W STORAGE  [initandlisten] Recovering data from the last clean checkpoint.
2018-03-01T11:08:50.950+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-03-01T11:08:51.878+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-03-01T11:08:51.878+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 17551 records totaling to 4345793 bytes
2018-03-01T11:08:51.878+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Mar  1 10:25:58:2 to determine where to place markers for truncation
2018-03-01T11:08:51.878+0800 I STORAGE  [initandlisten] Taking 2 samples and assuming that each section of oplog contains approximately 73917 records totaling to 18302545 bytes
2018-03-01T11:08:51.920+0800 I CONTROL  [initandlisten] 
2018-03-01T11:08:51.920+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-03-01T11:08:51.920+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-03-01T11:08:51.920+0800 I CONTROL  [initandlisten] 
2018-03-01T11:08:51.920+0800 I CONTROL  [initandlisten] 
2018-03-01T11:08:51.920+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-03-01T11:08:51.948+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-03-01T11:08:51.954+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-03-01T11:08:51.954+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-03-01T11:08:51.954+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-03-01T11:08:51.956+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-03-01T11:08:51.956+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-03-01T11:08:51.956+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-03-01T11:08:51.956+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-03-01T11:08:51.956+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-03-01T11:08:51.956+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-03-01T11:08:51.956+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-03-01T11:08:51.956+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-03-01T11:08:51.956+0800 I REPL     [rsSync] transition to RECOVERING
2018-03-01T11:08:51.956+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-03-01T11:08:51.956+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-03-01T11:08:51.957+0800 I REPL     [rsSync] transition to SECONDARY
2018-03-01T11:08:51.957+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-03-01T11:08:51.957+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T11:08:51.958+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-03-01T11:08:51.958+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-03-01T11:08:52.146+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50590 #1 (1 connection now open)
2018-03-01T11:08:52.146+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:50590 conn1: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:08:54.824+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50601 #2 (2 connections now open)
2018-03-01T11:08:54.824+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:50601 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:08:55.875+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50602 #3 (3 connections now open)
2018-03-01T11:08:55.875+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:50602 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:09:01.962+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state PRIMARY
2018-03-01T11:09:06.899+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:50647 #4 (4 connections now open)
2018-03-01T11:09:06.900+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:50647 conn4: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:09:06.996+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28002
2018-03-01T11:09:06.996+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-03-01T11:09:06.997+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-03-01T11:09:07.008+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-03-01T11:09:07.008+0800 I COMMAND  [conn4] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519873746000|1, t: 10 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 107ms
2018-03-01T11:09:07.009+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 1ms (2 connections now open to 127.0.0.1:28002)
2018-03-01T11:09:21.957+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-03-01T11:09:21.957+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-03-01T11:09:21.958+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-03-01T11:09:21.958+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-03-01T11:09:21.960+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-03-01T11:09:21.960+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-03-01T11:09:21.961+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-03-01T11:09:21.962+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-03-01T11:12:12.470+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-03-01T11:12:12.471+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-03-01T11:12:12.471+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-03-01T11:12:12.472+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-03-01T11:12:12.472+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-03-01T11:12:12.472+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-03-01T11:12:12.472+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-03-01T11:12:12.472+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-03-01T11:12:12.472+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28002: CallbackCanceled: Reporter no longer valid
2018-03-01T11:12:12.472+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-03-01T11:12:12.472+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-03-01T11:12:12.472+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28002 due to bad connection status; 1 connections to that host remain open
2018-03-01T11:12:12.473+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-03-01T11:12:12.475+0800 I -        [conn4] end connection 127.0.0.1:50647 (4 connections now open)
2018-03-01T11:12:12.475+0800 I -        [conn1] end connection 127.0.0.1:50590 (3 connections now open)
2018-03-01T11:12:12.476+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-03-01T11:12:12.476+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-03-01T11:12:12.477+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-03-01T11:12:12.477+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-03-01T11:12:12.480+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-03-01T11:12:13.006+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-03-01T11:12:13.007+0800 I CONTROL  [signalProcessingThread] now exiting
2018-03-01T11:12:13.007+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-03-01T11:12:13.007+0800 I CONTROL  [initandlisten] shutting down with code:0
2018-03-01T11:15:50.477+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-03-01T11:15:50.507+0800 I CONTROL  [initandlisten] MongoDB starting : pid=1807 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-03-01T11:15:50.507+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-03-01T11:15:50.507+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-03-01T11:15:50.507+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-03-01T11:15:50.507+0800 I CONTROL  [initandlisten] allocator: system
2018-03-01T11:15:50.507+0800 I CONTROL  [initandlisten] modules: none
2018-03-01T11:15:50.507+0800 I CONTROL  [initandlisten] build environment:
2018-03-01T11:15:50.507+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-03-01T11:15:50.507+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-03-01T11:15:50.507+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-03-01T11:15:50.510+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-03-01T11:15:50.510+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-03-01T11:15:51.410+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-03-01T11:15:51.410+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 17606 records totaling to 4358842 bytes
2018-03-01T11:15:51.410+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Mar  1 11:12:07:1 to determine where to place markers for truncation
2018-03-01T11:15:51.410+0800 I STORAGE  [initandlisten] Taking 2 samples and assuming that each section of oplog contains approximately 73927 records totaling to 18302630 bytes
2018-03-01T11:15:51.446+0800 I CONTROL  [initandlisten] 
2018-03-01T11:15:51.446+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-03-01T11:15:51.446+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-03-01T11:15:51.446+0800 I CONTROL  [initandlisten] 
2018-03-01T11:15:51.446+0800 I CONTROL  [initandlisten] 
2018-03-01T11:15:51.446+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-03-01T11:15:51.459+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-03-01T11:15:51.467+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-03-01T11:15:51.467+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-03-01T11:15:51.468+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-03-01T11:15:51.469+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-03-01T11:15:51.469+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-03-01T11:15:51.469+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-03-01T11:15:51.470+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-03-01T11:15:51.470+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-03-01T11:15:51.470+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-03-01T11:15:51.470+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-03-01T11:15:51.470+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-03-01T11:15:51.471+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-03-01T11:15:51.471+0800 I REPL     [rsSync] transition to RECOVERING
2018-03-01T11:15:51.471+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-03-01T11:15:51.471+0800 I REPL     [rsSync] transition to SECONDARY
2018-03-01T11:15:51.472+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T11:15:51.472+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-03-01T11:15:51.472+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-03-01T11:15:51.472+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-03-01T11:15:51.517+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51440 #1 (1 connection now open)
2018-03-01T11:15:51.518+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:51440 conn1: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:15:54.427+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51451 #2 (2 connections now open)
2018-03-01T11:15:54.427+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:51451 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:15:55.454+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51456 #3 (3 connections now open)
2018-03-01T11:15:55.455+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:51456 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:16:00.405+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51473 #4 (4 connections now open)
2018-03-01T11:16:00.405+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:51473 conn4: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:16:00.412+0800 I -        [conn2] end connection 127.0.0.1:51451 (4 connections now open)
2018-03-01T11:16:01.479+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-03-01T11:16:01.497+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28001
2018-03-01T11:16:01.497+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T11:16:01.498+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T11:16:01.498+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T11:16:01.499+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (2 connections now open to 127.0.0.1:28001)
2018-03-01T11:16:05.492+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51499 #5 (4 connections now open)
2018-03-01T11:16:05.492+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:51499 conn5: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:16:05.493+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51500 #6 (5 connections now open)
2018-03-01T11:16:05.493+0800 I NETWORK  [conn6] received client metadata from 127.0.0.1:51500 conn6: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:16:05.494+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:51501 #7 (6 connections now open)
2018-03-01T11:16:05.495+0800 I NETWORK  [conn7] received client metadata from 127.0.0.1:51501 conn7: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:16:21.469+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-03-01T11:16:21.469+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-03-01T11:16:21.470+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-03-01T11:16:21.470+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-03-01T11:16:21.471+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-03-01T11:16:21.471+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-03-01T11:16:21.472+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-03-01T11:16:21.472+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-03-01T11:17:12.925+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-03-01T11:17:12.926+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-03-01T11:17:12.926+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-03-01T11:17:12.926+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-03-01T11:17:12.926+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-03-01T11:17:12.926+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-03-01T11:17:12.926+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-03-01T11:17:12.926+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-03-01T11:17:12.926+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28001: CallbackCanceled: Reporter no longer valid
2018-03-01T11:17:12.926+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-03-01T11:17:12.926+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-03-01T11:17:12.927+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28001 due to bad connection status; 1 connections to that host remain open
2018-03-01T11:17:12.927+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-03-01T11:17:12.929+0800 I -        [conn7] end connection 127.0.0.1:51501 (6 connections now open)
2018-03-01T11:17:12.929+0800 I -        [conn1] end connection 127.0.0.1:51440 (5 connections now open)
2018-03-01T11:17:12.929+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-03-01T11:17:12.930+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-03-01T11:17:12.932+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-03-01T11:17:12.932+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-03-01T11:17:12.943+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-03-01T11:17:13.453+0800 I -        [conn3] end connection 127.0.0.1:51456 (4 connections now open)
2018-03-01T11:17:13.453+0800 I -        [conn5] end connection 127.0.0.1:51499 (4 connections now open)
2018-03-01T11:17:13.459+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-03-01T11:17:13.459+0800 I CONTROL  [signalProcessingThread] now exiting
2018-03-01T11:17:13.459+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-03-01T11:17:13.459+0800 I CONTROL  [initandlisten] shutting down with code:0
2018-03-01T11:36:32.380+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-03-01T11:36:32.406+0800 I CONTROL  [initandlisten] MongoDB starting : pid=3108 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-03-01T11:36:32.406+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-03-01T11:36:32.406+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-03-01T11:36:32.406+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-03-01T11:36:32.407+0800 I CONTROL  [initandlisten] allocator: system
2018-03-01T11:36:32.407+0800 I CONTROL  [initandlisten] modules: none
2018-03-01T11:36:32.407+0800 I CONTROL  [initandlisten] build environment:
2018-03-01T11:36:32.407+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-03-01T11:36:32.407+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-03-01T11:36:32.407+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-03-01T11:36:32.414+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-03-01T11:36:32.414+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-03-01T11:36:33.305+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-03-01T11:36:33.305+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 17629 records totaling to 4364279 bytes
2018-03-01T11:36:33.306+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Mar  1 11:17:11:1 to determine where to place markers for truncation
2018-03-01T11:36:33.306+0800 I STORAGE  [initandlisten] Taking 2 samples and assuming that each section of oplog contains approximately 73931 records totaling to 18302541 bytes
2018-03-01T11:36:33.351+0800 I CONTROL  [initandlisten] 
2018-03-01T11:36:33.351+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-03-01T11:36:33.352+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-03-01T11:36:33.352+0800 I CONTROL  [initandlisten] 
2018-03-01T11:36:33.352+0800 I CONTROL  [initandlisten] 
2018-03-01T11:36:33.352+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-03-01T11:36:33.388+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-03-01T11:36:33.393+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-03-01T11:36:33.393+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-03-01T11:36:33.394+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-03-01T11:36:33.395+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-03-01T11:36:33.395+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-03-01T11:36:33.395+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-03-01T11:36:33.395+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-03-01T11:36:33.395+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-03-01T11:36:33.396+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-03-01T11:36:33.396+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-03-01T11:36:33.396+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-03-01T11:36:33.396+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-03-01T11:36:33.396+0800 I REPL     [rsSync] transition to RECOVERING
2018-03-01T11:36:33.396+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-03-01T11:36:33.397+0800 I REPL     [rsSync] transition to SECONDARY
2018-03-01T11:36:33.397+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T11:36:33.397+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-03-01T11:36:33.397+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-03-01T11:36:33.397+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-03-01T11:36:33.585+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54123 #1 (1 connection now open)
2018-03-01T11:36:33.586+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:54123 conn1: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:36:36.288+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54134 #2 (2 connections now open)
2018-03-01T11:36:36.288+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:54134 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:36:37.356+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54135 #3 (3 connections now open)
2018-03-01T11:36:37.356+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:54135 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:36:41.654+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54151 #4 (4 connections now open)
2018-03-01T11:36:41.654+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:54151 conn4: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:36:41.661+0800 I -        [conn2] end connection 127.0.0.1:54134 (4 connections now open)
2018-03-01T11:36:43.401+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-03-01T11:36:43.439+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28001
2018-03-01T11:36:43.439+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T11:36:43.440+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T11:36:43.440+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T11:36:43.441+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (2 connections now open to 127.0.0.1:28001)
2018-03-01T11:36:47.405+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54173 #5 (4 connections now open)
2018-03-01T11:36:47.405+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:54173 conn5: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:36:47.407+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54174 #6 (5 connections now open)
2018-03-01T11:36:47.407+0800 I NETWORK  [conn6] received client metadata from 127.0.0.1:54174 conn6: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:37:02.505+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54215 #7 (6 connections now open)
2018-03-01T11:37:02.505+0800 I NETWORK  [conn7] received client metadata from 127.0.0.1:54215 conn7: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:37:03.395+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-03-01T11:37:03.395+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-03-01T11:37:03.396+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-03-01T11:37:03.398+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-03-01T11:37:03.399+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-03-01T11:37:03.400+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-03-01T11:37:03.402+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-03-01T11:37:03.402+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-03-01T11:40:30.810+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-03-01T11:40:30.815+0800 I -        [conn7] end connection 127.0.0.1:54215 (6 connections now open)
2018-03-01T11:40:30.815+0800 I -        [conn1] end connection 127.0.0.1:54123 (6 connections now open)
2018-03-01T11:40:30.834+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-03-01T11:40:30.834+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-03-01T11:40:30.834+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-03-01T11:40:30.834+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-03-01T11:40:30.918+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-03-01T11:40:30.918+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-03-01T11:40:30.918+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-03-01T11:40:30.918+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28001: CallbackCanceled: Reporter no longer valid
2018-03-01T11:40:30.918+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-03-01T11:40:30.918+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-03-01T11:40:30.918+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28001 due to bad connection status; 1 connections to that host remain open
2018-03-01T11:40:30.918+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-03-01T11:40:30.919+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-03-01T11:40:30.919+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-03-01T11:40:30.919+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-03-01T11:40:30.919+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-03-01T11:40:30.983+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-03-01T11:40:31.660+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-03-01T11:40:31.661+0800 I CONTROL  [signalProcessingThread] now exiting
2018-03-01T11:40:31.661+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-03-01T11:40:38.258+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-03-01T11:40:38.281+0800 I CONTROL  [initandlisten] MongoDB starting : pid=3352 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-03-01T11:40:38.281+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-03-01T11:40:38.281+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-03-01T11:40:38.281+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-03-01T11:40:38.281+0800 I CONTROL  [initandlisten] allocator: system
2018-03-01T11:40:38.281+0800 I CONTROL  [initandlisten] modules: none
2018-03-01T11:40:38.281+0800 I CONTROL  [initandlisten] build environment:
2018-03-01T11:40:38.281+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-03-01T11:40:38.281+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-03-01T11:40:38.281+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-03-01T11:40:38.282+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-03-01T11:40:38.282+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-03-01T11:40:39.251+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-03-01T11:40:39.251+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 17694 records totaling to 4379947 bytes
2018-03-01T11:40:39.267+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Mar  1 11:40:24:2 to determine where to place markers for truncation
2018-03-01T11:40:39.267+0800 I STORAGE  [initandlisten] Taking 2 samples and assuming that each section of oplog contains approximately 73938 records totaling to 18302504 bytes
2018-03-01T11:40:39.312+0800 I CONTROL  [initandlisten] 
2018-03-01T11:40:39.312+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-03-01T11:40:39.312+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-03-01T11:40:39.312+0800 I CONTROL  [initandlisten] 
2018-03-01T11:40:39.312+0800 I CONTROL  [initandlisten] 
2018-03-01T11:40:39.312+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-03-01T11:40:39.329+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-03-01T11:40:39.334+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-03-01T11:40:39.334+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-03-01T11:40:39.335+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-03-01T11:40:39.336+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-03-01T11:40:39.336+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-03-01T11:40:39.336+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-03-01T11:40:39.337+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-03-01T11:40:39.337+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-03-01T11:40:39.337+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-03-01T11:40:39.337+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-03-01T11:40:39.337+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-03-01T11:40:39.337+0800 I REPL     [rsSync] transition to RECOVERING
2018-03-01T11:40:39.337+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-03-01T11:40:39.337+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-03-01T11:40:39.338+0800 I REPL     [rsSync] transition to SECONDARY
2018-03-01T11:40:39.338+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 1ms (1 connections now open to 127.0.0.1:28002)
2018-03-01T11:40:39.339+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-03-01T11:40:39.339+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 2ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T11:40:39.339+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-03-01T11:40:42.190+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54649 #1 (1 connection now open)
2018-03-01T11:40:42.190+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:54649 conn1: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:40:43.240+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54654 #2 (2 connections now open)
2018-03-01T11:40:43.240+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:54654 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:40:46.202+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54679 #3 (3 connections now open)
2018-03-01T11:40:46.203+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:54679 conn3: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:40:48.349+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54689 #4 (4 connections now open)
2018-03-01T11:40:48.349+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:54689 conn4: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:40:48.355+0800 I -        [conn2] end connection 127.0.0.1:54654 (4 connections now open)
2018-03-01T11:40:49.345+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state PRIMARY
2018-03-01T11:40:54.262+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:54719 #5 (4 connections now open)
2018-03-01T11:40:54.263+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:54719 conn5: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:40:54.380+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28002
2018-03-01T11:40:54.380+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-03-01T11:40:54.382+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 2ms (1 connections now open to 127.0.0.1:28002)
2018-03-01T11:40:54.383+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28002
2018-03-01T11:40:54.384+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28002, took 1ms (2 connections now open to 127.0.0.1:28002)
2018-03-01T11:40:54.390+0800 I COMMAND  [conn5] command config.settings command: find { find: "settings", filter: { _id: "balancer" }, readConcern: { level: "majority", afterOpTime: { ts: Timestamp 1519875654000|1, t: 13 } }, limit: 1, maxTimeMS: 30000 } planSummary: EOF keysExamined:0 docsExamined:0 cursorExhausted:1 numYields:0 nreturned:0 reslen:354 locks:{ Global: { acquireCount: { r: 2 } }, Database: { acquireCount: { r: 1 } }, Collection: { acquireCount: { r: 1 } } } protocol:op_command 125ms
2018-03-01T11:41:09.338+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-03-01T11:41:09.339+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-03-01T11:41:09.339+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-03-01T11:41:09.340+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-03-01T11:41:09.340+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-03-01T11:41:09.341+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-03-01T11:41:09.341+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-03-01T11:41:09.342+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-03-01T11:41:53.687+0800 I CONTROL  [signalProcessingThread] got signal 15 (Terminated: 15), will terminate after current cmd ends
2018-03-01T11:41:53.687+0800 I NETWORK  [signalProcessingThread] shutdown: going to close listening sockets...
2018-03-01T11:41:53.689+0800 I NETWORK  [signalProcessingThread] closing listening socket: 7
2018-03-01T11:41:53.690+0800 I NETWORK  [signalProcessingThread] closing listening socket: 8
2018-03-01T11:41:53.690+0800 I NETWORK  [signalProcessingThread] removing socket file: /tmp/mongodb-28003.sock
2018-03-01T11:41:53.690+0800 I NETWORK  [signalProcessingThread] shutdown: going to flush diaglog...
2018-03-01T11:41:53.690+0800 I REPL     [signalProcessingThread] shutting down replication subsystems
2018-03-01T11:41:53.690+0800 I REPL     [signalProcessingThread] Stopping replication reporter thread
2018-03-01T11:41:53.692+0800 I REPL     [SyncSourceFeedback] SyncSourceFeedback error sending update to 127.0.0.1:28002: CallbackCanceled: Reporter no longer valid
2018-03-01T11:41:53.692+0800 I REPL     [signalProcessingThread] Stopping replication fetcher thread
2018-03-01T11:41:53.692+0800 I REPL     [signalProcessingThread] Stopping replication applier thread
2018-03-01T11:41:53.692+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending connection to host 127.0.0.1:28002 due to bad connection status; 1 connections to that host remain open
2018-03-01T11:41:53.692+0800 I REPL     [rsBackgroundSync] Replication producer stopped after oplog fetcher finished returning a batch from our sync source.  Abandoning this batch of oplog entries and re-evaluating our sync source.
2018-03-01T11:41:53.696+0800 I -        [conn5] end connection 127.0.0.1:54719 (4 connections now open)
2018-03-01T11:41:53.697+0800 I -        [conn3] end connection 127.0.0.1:54679 (3 connections now open)
2018-03-01T11:41:53.697+0800 I REPL     [signalProcessingThread] Stopping replication snapshot thread
2018-03-01T11:41:53.697+0800 I REPL     [signalProcessingThread] Stopping replication storage threads
2018-03-01T11:41:53.698+0800 W SHARDING [signalProcessingThread] cant reload ShardRegistry  :: caused by :: CallbackCanceled: Callback canceled
2018-03-01T11:41:53.698+0800 I FTDC     [signalProcessingThread] Shutting down full-time diagnostic data capture
2018-03-01T11:41:53.736+0800 I STORAGE  [signalProcessingThread] WiredTigerKVEngine shutting down
2018-03-01T11:41:54.361+0800 I -        [conn1] end connection 127.0.0.1:54649 (2 connections now open)
2018-03-01T11:41:54.367+0800 I STORAGE  [signalProcessingThread] shutdown: removing fs lock...
2018-03-01T11:41:54.368+0800 I CONTROL  [signalProcessingThread] now exiting
2018-03-01T11:41:54.368+0800 I CONTROL  [signalProcessingThread] shutting down with code:0
2018-03-01T11:41:54.368+0800 I CONTROL  [initandlisten] shutting down with code:0
2018-03-01T11:45:45.963+0800 I CONTROL  [main] ***** SERVER RESTARTED *****
2018-03-01T11:45:46.013+0800 I CONTROL  [initandlisten] MongoDB starting : pid=3688 port=28003 dbpath=/usr/local/mongo-cluster-sharding/configsvr/node3/data/db 64-bit host=kieren.local
2018-03-01T11:45:46.013+0800 I CONTROL  [initandlisten] db version v3.4.7
2018-03-01T11:45:46.013+0800 I CONTROL  [initandlisten] git version: cf38c1b8a0a8dca4a11737581beafef4fe120bcd
2018-03-01T11:45:46.013+0800 I CONTROL  [initandlisten] OpenSSL version: OpenSSL 0.9.8zg 14 July 2015
2018-03-01T11:45:46.013+0800 I CONTROL  [initandlisten] allocator: system
2018-03-01T11:45:46.013+0800 I CONTROL  [initandlisten] modules: none
2018-03-01T11:45:46.013+0800 I CONTROL  [initandlisten] build environment:
2018-03-01T11:45:46.013+0800 I CONTROL  [initandlisten]     distarch: x86_64
2018-03-01T11:45:46.013+0800 I CONTROL  [initandlisten]     target_arch: x86_64
2018-03-01T11:45:46.013+0800 I CONTROL  [initandlisten] options: { config: "/usr/local/mongo-cluster-sharding/configsvr/node3/mongodb.conf", net: { bindIp: "127.0.0.1", port: 28003 }, processManagement: { fork: true }, replication: { replSet: "configset" }, sharding: { clusterRole: "configsvr" }, storage: { dbPath: "/usr/local/mongo-cluster-sharding/configsvr/node3/data/db" }, systemLog: { destination: "file", logAppend: true, path: "/usr/local/mongo-cluster-sharding/configsvr/node3/log/mongodb.log" } }
2018-03-01T11:45:46.020+0800 I -        [initandlisten] Detected data files in /usr/local/mongo-cluster-sharding/configsvr/node3/data/db created by the 'wiredTiger' storage engine, so setting the active storage engine to 'wiredTiger'.
2018-03-01T11:45:46.020+0800 I STORAGE  [initandlisten] wiredtiger_open config: create,cache_size=3584M,session_max=20000,eviction=(threads_min=4,threads_max=4),config_base=false,statistics=(fast),log=(enabled=true,archive=true,path=journal,compressor=snappy),file_manager=(close_idle_time=100000),checkpoint=(wait=60,log_size=2GB),statistics_log=(wait=0),
2018-03-01T11:45:47.670+0800 I STORAGE  [initandlisten] Starting WiredTigerRecordStoreThread local.oplog.rs
2018-03-01T11:45:47.671+0800 I STORAGE  [initandlisten] The size storer reports that the oplog contains 17715 records totaling to 4384619 bytes
2018-03-01T11:45:47.674+0800 I STORAGE  [initandlisten] Sampling from the oplog between Feb 28 14:17:45:1 and Mar  1 11:41:52:1 to determine where to place markers for truncation
2018-03-01T11:45:47.674+0800 I STORAGE  [initandlisten] Taking 2 samples and assuming that each section of oplog contains approximately 73947 records totaling to 18302535 bytes
2018-03-01T11:45:47.818+0800 I CONTROL  [initandlisten] 
2018-03-01T11:45:47.818+0800 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2018-03-01T11:45:47.818+0800 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2018-03-01T11:45:47.818+0800 I CONTROL  [initandlisten] 
2018-03-01T11:45:47.818+0800 I CONTROL  [initandlisten] 
2018-03-01T11:45:47.818+0800 I CONTROL  [initandlisten] ** WARNING: soft rlimits too low. Number of files is 256, should be at least 1000
2018-03-01T11:45:47.866+0800 I FTDC     [initandlisten] Initializing full-time diagnostic data capture with directory '/usr/local/mongo-cluster-sharding/configsvr/node3/data/db/diagnostic.data'
2018-03-01T11:45:47.871+0800 I SHARDING [thread1] creating distributed lock ping thread for process ConfigServer (sleeping for 30000ms)
2018-03-01T11:45:47.871+0800 I SHARDING [shard registry reload] Periodic reload of shard registry failed  :: caused by :: 134 could not get updated shard list from config server due to Read concern majority reads are currently not possible.; will retry after 30s
2018-03-01T11:45:47.871+0800 I NETWORK  [thread2] waiting for connections on port 28003
2018-03-01T11:45:47.873+0800 I REPL     [replExecDBWorker-0] New replica set config in use: { _id: "configset", version: 1, configsvr: true, protocolVersion: 1, members: [ { _id: 1, host: "127.0.0.1:28001", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 2, host: "127.0.0.1:28002", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 }, { _id: 3, host: "127.0.0.1:28003", arbiterOnly: false, buildIndexes: true, hidden: false, priority: 1.0, tags: {}, slaveDelay: 0, votes: 1 } ], settings: { chainingAllowed: true, heartbeatIntervalMillis: 2000, heartbeatTimeoutSecs: 10, electionTimeoutMillis: 10000, catchUpTimeoutMillis: 60000, getLastErrorModes: {}, getLastErrorDefaults: { w: 1, wtimeout: 0 }, replicaSetId: ObjectId('5a9649899ac5bc9267871b60') } }
2018-03-01T11:45:47.873+0800 I REPL     [replExecDBWorker-0] This node is 127.0.0.1:28003 in the config
2018-03-01T11:45:47.873+0800 I REPL     [replExecDBWorker-0] transition to STARTUP2
2018-03-01T11:45:47.873+0800 I REPL     [replExecDBWorker-0] Starting replication snapshot thread
2018-03-01T11:45:47.873+0800 I REPL     [replExecDBWorker-0] Starting replication storage threads
2018-03-01T11:45:47.873+0800 I REPL     [replExecDBWorker-0] Starting replication fetcher thread
2018-03-01T11:45:47.873+0800 I REPL     [replExecDBWorker-0] Starting replication applier thread
2018-03-01T11:45:47.873+0800 I REPL     [replExecDBWorker-0] Starting replication reporter thread
2018-03-01T11:45:47.874+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28001
2018-03-01T11:45:47.874+0800 I REPL     [rsSync] transition to RECOVERING
2018-03-01T11:45:47.874+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Connecting to 127.0.0.1:28002
2018-03-01T11:45:47.874+0800 I REPL     [rsSync] transition to SECONDARY
2018-03-01T11:45:47.874+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28001, took 1ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T11:45:47.874+0800 I ASIO     [NetworkInterfaceASIO-Replication-0] Successfully connected to 127.0.0.1:28002, took 0ms (1 connections now open to 127.0.0.1:28002)
2018-03-01T11:45:47.875+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state SECONDARY
2018-03-01T11:45:47.875+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28002 is now in state SECONDARY
2018-03-01T11:45:48.827+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55266 #1 (1 connection now open)
2018-03-01T11:45:48.827+0800 I NETWORK  [conn1] received client metadata from 127.0.0.1:55266 conn1: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:45:49.627+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55271 #2 (2 connections now open)
2018-03-01T11:45:49.628+0800 I NETWORK  [conn2] received client metadata from 127.0.0.1:55271 conn2: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:45:50.945+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55276 #3 (3 connections now open)
2018-03-01T11:45:50.945+0800 I NETWORK  [conn3] received client metadata from 127.0.0.1:55276 conn3: { driver: { name: "NetworkInterfaceASIO-Replication", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:45:57.876+0800 I REPL     [ReplicationExecutor] Member 127.0.0.1:28001 is now in state PRIMARY
2018-03-01T11:45:57.906+0800 I REPL     [rsBackgroundSync] sync source candidate: 127.0.0.1:28001
2018-03-01T11:45:57.906+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T11:45:57.906+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 0ms (1 connections now open to 127.0.0.1:28001)
2018-03-01T11:45:57.907+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T11:45:57.908+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (2 connections now open to 127.0.0.1:28001)
2018-03-01T11:46:00.985+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55303 #4 (4 connections now open)
2018-03-01T11:46:00.985+0800 I NETWORK  [conn4] received client metadata from 127.0.0.1:55303 conn4: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:00.986+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55304 #5 (5 connections now open)
2018-03-01T11:46:00.986+0800 I NETWORK  [conn5] received client metadata from 127.0.0.1:55304 conn5: { driver: { name: "NetworkInterfaceASIO-RS", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:02.007+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55321 #6 (6 connections now open)
2018-03-01T11:46:02.007+0800 I NETWORK  [conn6] received client metadata from 127.0.0.1:55321 conn6: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:02.008+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55326 #7 (7 connections now open)
2018-03-01T11:46:02.009+0800 I NETWORK  [conn7] received client metadata from 127.0.0.1:55326 conn7: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:04.964+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55341 #8 (8 connections now open)
2018-03-01T11:46:04.965+0800 I NETWORK  [conn8] received client metadata from 127.0.0.1:55341 conn8: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:04.965+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55344 #9 (9 connections now open)
2018-03-01T11:46:04.965+0800 I NETWORK  [conn9] received client metadata from 127.0.0.1:55344 conn9: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:07.883+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55362 #10 (10 connections now open)
2018-03-01T11:46:07.883+0800 I NETWORK  [conn10] received client metadata from 127.0.0.1:55362 conn10: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:10.838+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55392 #11 (11 connections now open)
2018-03-01T11:46:10.838+0800 I NETWORK  [conn11] received client metadata from 127.0.0.1:55392 conn11: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:13.850+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55418 #12 (12 connections now open)
2018-03-01T11:46:13.850+0800 I NETWORK  [conn12] received client metadata from 127.0.0.1:55418 conn12: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:13.851+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55422 #13 (13 connections now open)
2018-03-01T11:46:13.851+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55423 #14 (14 connections now open)
2018-03-01T11:46:13.851+0800 I NETWORK  [conn13] received client metadata from 127.0.0.1:55422 conn13: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:13.851+0800 I NETWORK  [conn14] received client metadata from 127.0.0.1:55423 conn14: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:14.266+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55440 #15 (15 connections now open)
2018-03-01T11:46:14.266+0800 I NETWORK  [conn15] received client metadata from 127.0.0.1:55440 conn15: { driver: { name: "MongoDB Internal Client", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:14.268+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55442 #16 (16 connections now open)
2018-03-01T11:46:14.268+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55443 #17 (17 connections now open)
2018-03-01T11:46:14.268+0800 I NETWORK  [conn16] received client metadata from 127.0.0.1:55442 conn16: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:14.268+0800 I NETWORK  [conn17] received client metadata from 127.0.0.1:55443 conn17: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:17.875+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set1/127.0.0.1:27001,127.0.0.1:27002,127.0.0.1:27003
2018-03-01T11:46:17.875+0800 I NETWORK  [shard registry reload] Starting new replica set monitor for set2/127.0.0.1:27004,127.0.0.1:27005,127.0.0.1:27006
2018-03-01T11:46:17.876+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27003 (1 connections now open to 127.0.0.1:27003 with a 5 second timeout)
2018-03-01T11:46:17.877+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27001 (1 connections now open to 127.0.0.1:27001 with a 5 second timeout)
2018-03-01T11:46:17.877+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27002 (1 connections now open to 127.0.0.1:27002 with a 5 second timeout)
2018-03-01T11:46:17.878+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27006 (1 connections now open to 127.0.0.1:27006 with a 5 second timeout)
2018-03-01T11:46:17.879+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27005 (1 connections now open to 127.0.0.1:27005 with a 5 second timeout)
2018-03-01T11:46:17.879+0800 I NETWORK  [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to 127.0.0.1:27004 (1 connections now open to 127.0.0.1:27004 with a 5 second timeout)
2018-03-01T11:46:17.879+0800 W NETWORK  [ReplicaSetMonitor-TaskExecutor-0] No primary detected for set set2
2018-03-01T11:46:27.926+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55502 #18 (18 connections now open)
2018-03-01T11:46:27.926+0800 I NETWORK  [conn18] received client metadata from 127.0.0.1:55502 conn18: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:46:40.846+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55536 #19 (19 connections now open)
2018-03-01T11:46:40.846+0800 I NETWORK  [conn19] received client metadata from 127.0.0.1:55536 conn19: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
2018-03-01T11:47:13.858+0800 I -        [conn13] end connection 127.0.0.1:55422 (19 connections now open)
2018-03-01T11:47:14.273+0800 I -        [conn16] end connection 127.0.0.1:55442 (18 connections now open)
2018-03-01T11:47:23.091+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Connecting to 127.0.0.1:28001
2018-03-01T11:47:23.092+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Successfully connected to 127.0.0.1:28001, took 1ms (3 connections now open to 127.0.0.1:28001)
2018-03-01T11:48:23.094+0800 I ASIO     [NetworkInterfaceASIO-RS-0] Ending idle connection to host 127.0.0.1:28001 because the pool meets constraints; 2 connections to that host remain open
2018-03-01T11:50:37.922+0800 I NETWORK  [thread2] connection accepted from 127.0.0.1:55932 #20 (18 connections now open)
2018-03-01T11:50:37.977+0800 I NETWORK  [conn20] received client metadata from 127.0.0.1:55932 conn20: { driver: { name: "NetworkInterfaceASIO-ShardRegistry", version: "3.4.7" }, os: { type: "Darwin", name: "Mac OS X", architecture: "x86_64", version: "15.3.0" } }
